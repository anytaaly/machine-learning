{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKcm/4SHsfsiorNc1nj53z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anytaaly/machine-learning/blob/main/Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyPVM_Y33Jou"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and Probability Review\n",
        "This module will introduce the main idea behind statistical learning for data science. You will learn to differentiate between model-driven and data-driven approaches to address complex problems, as well as supervised and unsupervised learning techniques. We will start by providing a crash course on probability tools that will be needed throughout the course, such as probability spaces, random variables, probability distributions, expectations, Bayes’ rule, and multivariate probability. Finally, we will introduce the programming language Python, which will be used to illustrate our theoretical advances throughout the course.\n",
        "\n",
        "# Learning Objectives\n",
        "\n",
        "Understand the difference between model-driven and data-driven approaches to\n",
        "\n",
        "*   Understand the difference between model-driven and data-driven approaches to address complex problems.\n",
        "*   Understand the difference between supervised and unsupervised learning problems.\n",
        "Remember the probability tools to study statistical learning problems in future modules.\n",
        "*  Install and explore the Python programming language using notebooks."
      ],
      "metadata": {
        "id": "xr4w4LmL-EF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic modeling** is a mathematical approach to represent and predict systems or phenomena that involve an element of randomness or chance"
      ],
      "metadata": {
        "id": "6fIznBsrBsHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "This course will not focus on programming or implementation, for which you can find excellent courses online, but on developing an understanding about why, how and when some of these learning techniques work from a statistical perspective.\n",
        "\n",
        "Choosing the right learning technique to implement in your code must be based on an understanding of the tool you're implementing, instead of blindly trying different alternatives.\n",
        "\n",
        "On the other hand, those making decisions from data must understand the scope and limitations of the techniques used to digest this data in order to make informed and valuable decisions.\n"
      ],
      "metadata": {
        "id": "-pXQhY95BwQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data science is about drawing useful conclusions from large and diverse data sets through three phases, which I described here as exploration, prediction and inference."
      ],
      "metadata": {
        "id": "V1qGIDopCwsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Exploration Phase:\n",
        "In the exploration phase, we try to identify patterns that are useful in our analysis using visualization and descriptive statistics.\n",
        "Also known as Exploratory Data Analysis (EDA), it helps uncover insights, identify anomalies and outliers, formulate hypotheses, and select appropriate analytical methods before building models or making conclusions."
      ],
      "metadata": {
        "id": "vdZuuU-LKg97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2- Planning Phase:\n",
        "In a second phase, which I call prediction, we use information to make informed guesses about values we wish we knew using machine learning and other tools."
      ],
      "metadata": {
        "id": "uv0zPM4nK1gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Inference\n",
        "In our third phase called inference. We quantify the degree of certainty or uncertainty in our models. In other words, we try to answer the question, how accurate are our predictions? The main tools in this third phase are statistical tests and models.\n",
        "\n",
        "In data science, inference refers to the process of using data analysis and statistical methods to draw conclusions about a larger population or system based on a sample of data."
      ],
      "metadata": {
        "id": "sMYBAipqLGyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics is a central component in data science because it studies how to make robust conclusions based on incomplete information, and this will be the focus of this course."
      ],
      "metadata": {
        "id": "fK-mR3VxLjUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8p_GsCYWCZI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data-Driven Learning\n",
        "**1. Model-Based Approach (First-Principles Driven)**\n",
        "\n",
        "Definition: This approach relies on theoretical models derived from established scientific principles (e.g., Newton’s laws of motion, Maxwell’s equations, Einstein’s relativity).\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "Provides explainability (why something happens).\n",
        "\n",
        "Models often have predictive power in well-defined domains (physics, chemistry, engineering).\n",
        "\n",
        "Require relatively less data since the governing equations are already known.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "Breaks down in complex, chaotic, or poorly understood systems where exact governing laws are unknown or too difficult to model (e.g., climate change in detail, human behavior, stock markets).\n",
        "\n",
        "**2. Data-Driven Approach (Empirical / Machine Learning)**\n",
        "\n",
        "Definition: Instead of relying on known physical laws, this approach uses data itself to infer patterns, correlations, and predictive models.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Stock market prediction (too many hidden variables).\n",
        "\n",
        "Human psychology and behavior modeling.\n",
        "\n",
        "Brain activity and neural processing.\n",
        "\n",
        "Strengths:\n",
        "\n",
        "Can uncover patterns in systems that are too complex or nonlinear for first-principle models.\n",
        "\n",
        "Improves as more data becomes available (big data + machine learning).\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Often lacks interpretability—models may be “black boxes.”\n",
        "\n",
        "Requires large amounts of high-quality data.\n",
        "\n",
        "May capture correlation, not causation.\n",
        "These systems are so complex that we don't have the Newton slow equivalent.\n"
      ],
      "metadata": {
        "id": "eqxwbhZyBIdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1WXt-RLUhV3kjLWHZTtSgYC2u3qTyS1vX)\n"
      ],
      "metadata": {
        "id": "mZfmM2oDSsgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1gYRBIsnNuwrnGQ3khC2Ptkd39v4Df0PZ)\n"
      ],
      "metadata": {
        "id": "JDge16MgF7i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "X4huo1_HGj_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Consider a collection of 1000 gray-scale images of dogs and cats. Each image has a resolution of 32 x 32 pixels, and each pixel has 256 possible gray levels. Your learning problem is to say what is the animal in an image. In this scenario, what is the number of features p and the number of data points N ?*\n",
        "\n",
        "Looking at this machine learning scenario, let me break down the components:\n",
        "Number of data points (N):\n",
        "N = 1000\n",
        "This is simply the total number of images in your collection.\n",
        "Number of features (p):\n",
        "Each image has 32 × 32 = 1,024 pixels, and if we treat each pixel as a feature, then:\n",
        "p = 1,024\n",
        "Each pixel can take one of 256 possible gray levels (typically 0-255), but the number of features is determined by the dimensionality of the input space, not the number of possible values each feature can take.\n",
        "So in summary:\n",
        "\n",
        "N = 1,000 (data points)\n",
        "p = 1,024 (features)\n",
        "\n",
        "This gives you a scenario where p ≈ N, which is interesting from a machine learning perspective since you're in a regime where the number of features is comparable to the number of training examples. This can present challenges like overfitting and may require techniques like regularization, dimensionality reduction, or data augmentation to achieve good generalization performance."
      ],
      "metadata": {
        "id": "LeXYU7DPVkKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topics Covered"
      ],
      "metadata": {
        "id": "IEhC08UFULbI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "1.   Introduction\n",
        "2.   Statistical Learning\n",
        "3.   Linear Regression\n",
        "4.   Classification\n",
        "5.   Resampling Methods\n",
        "6.   Model Selection and Regularization\n",
        "7.   Tree-Based Methods\n",
        "8.   Support Vector Machines\n",
        "9.   Unsupervised Learning\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eOMH7fdNUO_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All the topics mentioned in the previous slide can be classified using two families of learning problems.\n",
        "\n",
        "# Supervised Learning:\n",
        "The first family is what's called **supervised learning**. In a supervised learning problem, we are given a training data set D. Inside this data set, we have a collection of N pairs. Each pair, $ x_i $  and $ y_i $ , represents two elements.\n",
        "\n",
        "\n",
        "*   Given:\n",
        "A training dataset D = $ {(x_1,y_1), ...., (x_N,y_N)}   $ Where\n",
        "$ x_i  $ are p-dimensional inputs (aka features, regressors, covariates)\n",
        "$ y_i $ are outcomes variables (ala response, target, dependent variables)\n",
        "\n",
        "The first element, x_i, is a p-dimensional input.\n",
        "A p-dimensional input is nothing but a collection of features that we can use to classify the input. So think of, for example, if this is an image, x_i can be a collection of the pixels. If it's a voice recording, would be the voice signal or some features about this voice signals such as the power spectral density and so on and so forth.\n",
        "\n",
        "**What is a feature in Machine Learning?**\n",
        "\n",
        "A feature is an individual measurable property of your data that helps the model make predictions or decisions.\n",
        "\n",
        "Think of features as the inputs (the “X” values) that describe each data point.\n",
        "\n",
        "Features are what the algorithm “looks at” to learn patterns and make classifications or predictions.\n",
        "\n",
        "\n",
        "\n",
        "**Example 1: Image Recognition**\n",
        "\n",
        "Suppose you have a 32 × 32 grayscale image. Each pixel value (0–255 for brightness) is one feature.\n",
        "\n",
        "Total features =\n",
        "32 × 32 = 1024\n",
        "\n",
        "So, when you train your model, every image is represented as a vector of 1024 features.\n",
        "![](https://drive.google.com/uc?export=view&id=1tbbSKZ5A27nwtP5dinyBsVKOe7d7OQ-d)\n",
        "\n",
        "\n",
        "**Example 2: Audio Classification**\n",
        "\n",
        "You have a 2-second audio recording sampled at 100 Hz. That means 200 samples (like little points of sound intensity).\n",
        "\n",
        "Each sample is a feature → total = 200 features.\n",
        "\n",
        "\n",
        "**Example 3: Predicting House Prices**\n",
        "\n",
        "If you’re predicting the price of a house, some features might be:\n",
        "\n",
        "Size of the house (in square feet)\n",
        "\n",
        "*   Number of bedrooms\n",
        "*   Location (zip code)\n",
        "*   Age of the house\n",
        "*   Whether it has a garage\n",
        "\n",
        "\n",
        "Here, each of these is a feature that describes the house.\n",
        "\n",
        "**In summary**\n",
        "\n",
        "Features = inputs that describe your data point.\n",
        "\n",
        "Labels = outputs you want to predict (e.g., cat/dog, price, phoneme type).\n",
        "\n",
        "The number of features = the length of the vector describing one data point.\n",
        "\n",
        "## 🔹 What is a Vector?\n",
        "\n",
        "At its simplest, a vector is just an ordered list of numbers. Each number represents a feature. The vector represents one data point in your dataset.\n",
        "\n",
        "The variable $ y_i $ is what's called the outcome or the output, also called response target or dependent variable. This $ y_i $ is typically either a value or a label that we want to assign to the input $ x_i $.\n",
        "In practice, we want to find a function able to map inputs to outcome variables.\n",
        "As I mentioned before, the outcome variable can be of two types. If $ y_i $ is a quantitative variable, we say that the problem is called a **regression problem**, think of quantitative variables such as salary. In contrast, the outcome can be a qualitative variable. In that case, the problem is called a **classification problem**.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Statistical learning refers to a vast set of tools for understanding data.\n",
        "these tools can be classified as *supervised* or *unsupervised*.\n",
        "\n",
        "Broadly speaking, supervised statistical learning involves building a statistical model for predicting, or estimating, an output based on one or more inputs.\n",
        "\n"
      ],
      "metadata": {
        "id": "yZwZnvS4UpQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# 🔹 Input vs. Output in ML\n",
        "\n",
        "\n",
        "*   Each data point is described by input features -> denoted as $ x_i $\n",
        "*   The output / label we want to predict is $ y_i $\n",
        "*   The gial of ML is to learn a function\n",
        "\n",
        "$$\n",
        "f(x_i) \\approx y_i\n",
        "$$\n",
        "\n",
        "so that given a new input, the model can predict its outcome.\n",
        "\n",
        "\n",
        "🔹 Two Main Types of Outcome Variables\n",
        "\n",
        "**1. Regression Problems**\n",
        "\n",
        "When $ y_i $ is quantitative (numeric, continuous).\n",
        "\n",
        "Example:\n",
        "\n",
        "* Predicting salary of an employee ($45,000, $67,000, etc.)\n",
        "\n",
        "* Predicting house price ($120k, $300k, …)\n",
        "\n",
        "* Predicting temperature tomorrow (23.5°C, 30.2°C, …)\n",
        "\n",
        "The model outputs a real number.\n",
        "\n",
        "**2. Classification Problems**\n",
        "\n",
        "When  $ y_i $ is qualitative (categorical, discrete).\n",
        "\n",
        "Example:\n",
        "\n",
        "* Predicting if an email is spam or not spam (labels: 0, 1).\n",
        "\n",
        "* Predicting whether an animal in an image is dog or cat (labels: dog, cat).\n",
        "\n",
        "* Predicting the customer’s sentiment: positive, neutral, or negative.\n",
        "\n",
        "The model outputs a class label."
      ],
      "metadata": {
        "id": "s0eJsA2PK-Pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "🔹 What does p-dimensional mean?\n",
        "\n",
        "It means that each data point is described by p numbers (features).\n",
        "\n",
        "**🏠 Example: House Price Prediction**\n",
        "\n",
        "Suppose each house has 3 features:\n",
        "\n",
        "* Size (in sq ft)\n",
        "\n",
        "* Number of bedrooms\n",
        "\n",
        "* Age of the house\n",
        "\n",
        "Then each house (one data point) can be written as:\n",
        "\n",
        "$ 𝑥_𝑖 = [ 2000 , 3 ,10 ] $\n",
        "\n",
        "Here, p=3.\n",
        "So, the input space is 3-dimensional."
      ],
      "metadata": {
        "id": "43XbLx53NO0V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1e8ST6Et49YqgmsYUd4GyzNsdz4nItsIE)"
      ],
      "metadata": {
        "id": "d5fMlYo2TnBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Unsupervised Learning:\n",
        "\n",
        "Apart from supervised learning, we also have unsupervised learning problems. In this type of problems, we are given a collection of inputs alone. So instead of pairs, as we had before, we're only given the inputs $ { x_1 - x_N } $.\n",
        "\n",
        "Our problem is to find groups of variables that behaves similarly.\n",
        "\n",
        "For example, any clustering problem in which we are not given labeled data would be an unsupervised learning problem.\n",
        "\n",
        "\n",
        "With unsupervised statistical learning, there are inputs but no supervising output; nevertheless we can learn relationships and structure from such data."
      ],
      "metadata": {
        "id": "eOou8hHOUYcq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability Theory:\n",
        "Probability theory is the study of uncertainty. Through this class, we will be relying on concepts from probability theory for deriving machine learning algorithms. These notes attempt to cover the basics of probability theory at a level appropriate for CS 229.\n",
        "\n",
        "The mathematical theory of probability is very sophisticated, and delves into a branch of analysis known as measure theory. In these notes, we provide a basic treatment of probability that does not address these finer details.\n",
        "\n",
        "# Elements of Probability:\n",
        "In order to define a probability on a set we need a few basic elements:\n",
        "The building blocks of probability theory:\n",
        "\n",
        "1.   **Sample Space: $ \\Omega $**\n",
        "\n",
        "The set of all the possible outcomes of a random experiment. Here, each outcome\n",
        "$ \\omega \\in \\Omega$  can be thought of as a complete description of the state of the real world at the end of the experiment.\n",
        "\n",
        "Example: When rolling two dice: Each die has outcomes  {1,2,3,4,5,6}.\n",
        "\n",
        "The sample space is all ordered pairs: S = {(i,j):i∈{1,…,6},j∈{1,…,6}}\n",
        "\n",
        "So |S| = 36.\n",
        "\n",
        "Each element ($ \\omega \\in \\Omega$) is a complete description of what could happen in one trial of the experiment.\n",
        "\n",
        "\n",
        "2.   **Set of events: $ $**\n",
        "\n",
        "An event is a subset of the sample space.\n",
        "\n",
        "Example: Simple Events: Rolling(2,5) & A = {(2,5)}\n",
        "\n",
        "3.   **Event Space: $ $**\n",
        "\n",
        "Since the sample space is finite (36 elements), the event space is the power set of S. That means:\n",
        "$$ \\mathscr{F}= {A: A \\subseteq S } $$\n",
        "\n",
        "Number of possible events: $2^{36}$ (a huge number, ~ 68 billion!).\n",
        "Includes:\n",
        "* $\\emptyset$ impossible event.\n",
        "* $ S $ (Certain event).\n",
        "* Any subset of outcomes (like events A,B,C,D above).\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1J-lRVheikuSGyFAAsD4TdLnLHdiPDs3G)\n",
        "\n",
        "\n",
        "4.   **Probabilities measure:**\n",
        "\n",
        "This is a function that assigns probabilities to events. It must satisfy the axioms of probability:\n",
        "\n",
        "So yes — every event in F gets a probability assigned to it. ✅\n",
        "\n",
        "✅ So the refined answer is:\n",
        "\n",
        "In finite/ countable cases: yes, all subsets of outcomes are events, and each gets a probability.\n",
        "\n",
        "In continuous cases: only measurable sets (those in the σ-algebra) get assigned probabilities, not every conceivable subset.\n",
        "\n",
        "## Properties of Probability.\n",
        "\n",
        "**4.1- Non-negativity:**\n",
        "\n",
        "$ 𝑃 (𝐴)  \\geq 0 $ , $ \\forall A \\in 𝐅 $  \n",
        "\n",
        "→ Probabilities can’t be negative.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1f4RZJYYLoAVyh8GIGSdLOXQ37z_EB-DT)\n",
        "\n",
        "**4.2- Normalization:**\n",
        "\n",
        "$ P(\\Omega)=1 $\n",
        "\n",
        "→ The probability of the entire sample space (something in Ω must happen) = 1\n",
        "\n",
        "**4.3- Additivity (for disjoint events):**\n",
        "\n",
        "If $A_1$ , $A_2$, .. are disjoint (they don't overlap then:\n",
        "\n",
        "$ Pr (A_1 ​\\bigcup A_2) = Pr(A_1) + Pr(A_2) $\n",
        "\n",
        "Or\n",
        "\n",
        "$$ P(​\\bigcup_i A_i​)= \\sum_i  P(A_i​) $$\n",
        "\n",
        "**4.4- probability of the intersection of two sets A and B:**\n",
        "$$ Pr (A \\cap B) \\leq min(Pr(A) , Pr(B)) $$\n",
        "\n",
        "The probability of the intersection of two sets A and B, is less or equal than the minimum between the probability of A and the probability of B.\n",
        "\n",
        "**5-:**\n",
        "$$ Pr (A \\cap A) \\leq Pr(A) + Pr(B) $$\n",
        "The second property states that the probability of the union is less or equal than the sum of the probabilities, probability of A and probability of B.\n",
        "\n",
        "**6-:**\n",
        "If $A_1,....,A_k $ are a partition of $\\Omega $, then $ \\sum_{i=1}^{k} Pr(A_i) = 1$\n",
        "\n",
        "What does “partition of Ω” mean?\n",
        "\n",
        "A partition of the sample space Ω means:\n",
        "\n",
        "* The events $A_1, A_2, .... A_k $  are disjoint (no overlap,$ A_i\n",
        "\\cap A_j =  \\varnothing $ for $ i \\neq j $)\n",
        "\n",
        "\n",
        "\n",
        "* Together they cover the whole sample space ($A_1 \\cup A_2 \\cup A_3 \\cup A_k = \\Omega $)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1D5lvGMupl9lqrNKZRZFgASfGXuL5IsQF)\n",
        "\n",
        "\n",
        "✅ In short:\n",
        "If you break the entire sample space into mutually exclusive events (no overlaps) that cover all possibilities, then the probabilities of those events must add up to 1, because something in Ω must happen."
      ],
      "metadata": {
        "id": "2tWD9XE6V5LR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Probability and independence:\n"
      ],
      "metadata": {
        "id": "m0PAkyzaIw_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conditional probability measures the probability of an event occurring given that another event has already occurred or is known to be true.\n",
        "The conditional probability of any event A given an event B is defined as,\n",
        "\n",
        "$$ Pr (A|B) = \\frac{Pr(A \\cap B)}{Pr(B)} $$\n",
        "\n",
        "\n",
        "* $A\\cap B$ = “both A and B happen.”\n",
        "\n",
        "* $P(A \\cup  B)$ = probability that both events occur.\n",
        "\n",
        "* We divide by P(B) because we are restricting the world to situations where B has already occurred.\n",
        "\n",
        "In plain words, $ Pr(A|B) $ represents the probability of event A after observing the occurence of event B.\n",
        "\n",
        "Two evens are called independent if and only if:\n",
        "$$ Pr (A \\cap B) - Pr(A) Pr(B)  $$\n",
        "or equivalenty, Pr(A|B) = Pr(A)\n",
        "\n",
        "\n",
        "✅ In simple words: Conditional probability zooms into a smaller “world” (where\n",
        "B has happened) and asks: within that smaller world, how likely is A?"
      ],
      "metadata": {
        "id": "jx5YtHNgI22W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If A and B are independent, then\n",
        "\n",
        "$ P(A∣B)=P(A) $\n",
        "\n",
        "(knowing  B doesn’t change the probability of A).\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mZcn-EeVF8adpSudKAJmtceYgfY3Ju9t)\n",
        "\n"
      ],
      "metadata": {
        "id": "VUAemyzHNPib"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Haa6B33dNqgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Variable"
      ],
      "metadata": {
        "id": "zf_-y6-yOCiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despite the name, a random variable is not “random” and not exactly a “variable” in the usual sense.\n",
        "\n",
        "👉 A random variable (RV) is a function that assigns a number to each outcome of a random experiment.\n",
        "\n",
        "The experiment produces an outcome ω (an element of the sample space Ω).\n",
        "\n",
        "The random variable maps ω to a real number.\n",
        "\n",
        "Formally:\n",
        "\n",
        "$$ 𝑋 : Ω → R $$\n",
        "\n",
        "🔹 Why do we need Random Variables?\n",
        "The sample space Ω can be complicated (words, colors, faces, sounds, …).\n",
        "It’s easier to work with numbers.\n",
        "So we define a random variable that converts outcomes into numerical values we can analyze.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=14V6KEmVdMTfs_qN56XUeDxTS2AtfvcUP)\n",
        "\n"
      ],
      "metadata": {
        "id": "QsWlbcDzPEdg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🔹 Types of Random Variables\n",
        "\n",
        "**Discrete Random Variable**\n",
        "\n",
        "* Takes values from a countable set.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Die roll = {1,2,3,4,5,6}\n",
        "* Number of cars passing in 1 hour = {0,1,2,…}\n",
        "* Coin toss indicator = {0,1}\n",
        "* Number of Emergency call in an hour\n",
        "* number of bulbs that produces flower\n",
        "\n",
        "**Continuous Random Variable**\n",
        "\n",
        "Takes values from an interval of real numbers.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* Height of a person (e.g., 165.2 cm)\n",
        "* Temperature tomorrow (e.g., 23.6 °C)\n",
        "* Time until next bus arrives\n",
        "* Weight of a Suitcase\n",
        "* time taken to get to a fire\n",
        "* Length of a tulip stem\n",
        "\n",
        "Good Youtube: https://www.youtube.com/watch?v=lHCpYeFvTs0&t=81s\n"
      ],
      "metadata": {
        "id": "ct68-bA0QVx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probability Distribution Functions (PDFs)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1nqfXvo-XKGTqljcTfb5WhWkQEy5kOXt5)"
      ],
      "metadata": {
        "id": "9xLRFIcTZhzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probablity Mass Function\n",
        "\n",
        "The Probability Mass Function applies to discrete random variables.(Example of Discrete Variables:\n",
        "\n",
        "\n",
        "👉 It gives the probability that the random variable equals some value:\n",
        "$$ P (x) = Pr( X=x)  $$\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1bPEOaNCGUPLgBLeNaZ1YHD9yUwohUI1m)\n",
        "Cumulative probability will become of probablity of rolling 4 or less.\n",
        "\n",
        "One of the properties of cumulative function is that the final bar needs to be 1, for example. the probablity of getting 6 or less is 100%. you can't get a roll a 7 on any dice.\n",
        "\n",
        "\n",
        "Reference video: https://www.youtube.com/watch?v=YXLVjCKVP7U&t=16s\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UmNqfknDZnuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "🔹 How to Find a PMF (General Strategy)\n",
        "\n",
        "Whenever you’re asked “What is the PMF of X?”:\n",
        "\n",
        "1- Understand the random variable (RV).\n",
        "\n",
        "* What does X represent?\n",
        "\n",
        "* Is it discrete or continuous?\n",
        "\n",
        "* What values can it take?\n",
        "\n",
        "2- List the sample space (Ω).\n",
        "\n",
        "* Write all possible outcomes of the experiment.\n",
        "\n",
        "3- Map outcomes to values of X.\n",
        "\n",
        "* For each outcome, compute the value of X.\n",
        "\n",
        "4- Count frequencies or compute probabilities.\n",
        "\n",
        "* For a discrete RV, the PMF is:\n",
        "\n",
        "$ p(x) = P(X = x) = \\frac{\\text{# of favorable outcomes for } X=x}{\\text{total outcomes}} $\n",
        "\n",
        "Check two things:\n",
        "\n",
        "Non-negativity:\n",
        "$ p(x)\\geq 0 $\n",
        "\n",
        "Normalization:\n",
        "$ \\sum p(x)=1 $"
      ],
      "metadata": {
        "id": "h1_TFm2krm_i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1.1: #\n",
        "\n",
        "\n",
        "---\n",
        "Consider the experiment of flipping three coins. What is the size of the sample space\n",
        "\n",
        "Each coin has 2 outcomes. There are 3 coins. $|S|$ = $2^3$\n",
        "\n",
        "# Question 1.2: #\n",
        "\n",
        "\n",
        "---\n",
        "Consider the experiment of flipping three coins. Assuming the coins are all fair, what is the probability of observing exactly two heads and one tail (in any order)? Input your answer as an irreducible fraction p/q, where p and q are integer numbers. For example, if your answer is 0.6, enter 3/5.\n",
        "\n",
        "{(H,H,H), (H,H,T), (H,T,H), (T,H,H) , (T,H,H) ,(T,H,T), (T,T,H), (T,T,T)}\n",
        "\n",
        "3/8\n"
      ],
      "metadata": {
        "id": "9DO4p0Di-wgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Question 2.1:\n",
        "---\n",
        "\n",
        "Q: Consider the experiment of flipping three fair coins. Define the random variable H as the number of heads observed in the experiment. What is the PMF of H ?\n",
        "\n",
        "**Step 1: What values can X take?**\n",
        "\n",
        "Minimum = 0 (no heads at all).\n",
        "\n",
        "Maximum = 3 (all heads).\n",
        "So possible values:  $ X \\in {0,1,2,3} $\n",
        "\n",
        "**Step 2: Write sample space (Ω).**\n",
        "\n",
        "For 3 coin flips:\n",
        "Ω = {HHH, HHT, HTH, THH, HTT, THT, TTH, TTT}\n",
        "Total outcomes = 8\n",
        "\n",
        "**Step 3: Map outcomes to X**\n",
        "\n",
        "X=3: {HHH} → 1 outcome.\n",
        "\n",
        "X=2: {HHT, HTH, THH} → 3 outcomes.\n",
        "\n",
        "X=1: {HTT, THT, TTH} → 3 outcomes.\n",
        "\n",
        "X=0: {TTT} → 1 outcome.\n",
        "\n",
        "**Step 4: Compute probabilities.**\n",
        "\n",
        "Each outcome has probability 1/8 (fair coin).\n",
        "\n",
        "So:\n",
        "\n",
        "P(X=0) = 1/8, P(X=1) = 3/8 , P(X=2) = 3/8, P(X=3) = 1/8\n",
        "\n",
        "**Step 5: Check sum = 1.**\n",
        "1/8 + 3/8 + 3/8 + 1/8 = 1\n"
      ],
      "metadata": {
        "id": "Mr49VUeJslOM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ri_hsIYc-nj6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Probability Density function (PDF)\n",
        "\n",
        "Reference video: https://www.youtube.com/watch?v=oI3hZJqXJuc\n",
        "\n",
        "Probability Distribution Function: https://www.youtube.com/watch?v=YXLVjCKVP7U&t=144s\n",
        "\n"
      ],
      "metadata": {
        "id": "mpwtfcsJgj_g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1VZVhOYRiR9LxW4iBRk6ACxAvMOXsaUP5)\n"
      ],
      "metadata": {
        "id": "iMBlrDgSg7Xi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conditional Probablity Density Function\n",
        "\n",
        "$f_{Y|X} (y|x) $\n",
        "is the conditional probability density function (PDF) of Y given that X = x.\n",
        "\n",
        "It describes the distribution of Y once you fix the value of X.\n",
        "So instead of asking \"*what's is te probablility of Y = y in general?*\"- you are asking \"*whats the probability of Y = y given X = x*\"?\n",
        "\n",
        "\n",
        "\n",
        "### 2. Formal Definition\n",
        "\n",
        "Conditional density is defined using the joint density $ f_{x,y} $ and the marginal density $ f_X(x)$\n",
        "\n",
        "\n",
        "$$ f_{Y|X} (y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)} for F_X(x) > 0 $$\n"
      ],
      "metadata": {
        "id": "6VJovh47jMLL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Expectation"
      ],
      "metadata": {
        "id": "LzSXuxEYs3zf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "The expected value (or expectation) of a probability distribution is the long-term average of the outcomes if the random experiment were repeated many times.\n",
        "\n",
        "Expected value is the weighted average in probablity, it multiplies each outcome by its chance. Think of it as balancing by all possible outcomes. This method predicts long term results from random events.\n",
        "\n",
        "It tells you the usual results over many trials.\n",
        "\n",
        "In sports, it can predict win chances and score.\n",
        "\n",
        "In finance, it balances risk with potential risk.\n",
        "\n",
        "\n",
        "For a **discrete distribution**, it's calculated by summing the product of each possible value and its probability (Σ xP(x)).\n",
        "\n",
        "Given a discrete random variable X with PMF px and a function  $g:  \\mathbb{R} \\to \\mathbb{R} $, the expectations ( or expected value) of g(X) is defined as:\n",
        "\n",
        "$$ \\mathbb{E}[g(X)] = \\sum_{x \\in \\mathscr{x}}  g(x) \\cdot  px(x)   $$\n",
        "\n",
        "Example:\n",
        "So think of the case of X being the throw of a dice and the function g could be something like the squared function.\n",
        "$$ X \\to Dice   $$\n",
        "$$ g(x) x^2   $$\n",
        "So this is a function that map's real numbers into real numbers.\n",
        "$$ \\mathbb{E}[g(X)] = \\mathbb{E}[X^2]  $$\n",
        "\n",
        "So remember the argument in here is the X, which is the throw of a dice i.e outcome of rolling a fair 6-sided die .\n",
        "and we know that the p of x is equal to 1/6 for all possible outcomes of the dice, right?\n",
        "$$ = \\sum_{x \\in \\mathscr{x}}  x^2  \\cdot  1/6 $$\n",
        "\n",
        "where $\\mathscr{x}$ is the values that the dice can take and this is nothing but the set 1, 2, up to 6.\n",
        "\n",
        "$$  \\mathscr{x} = { 1, 2, 3, . . . , 6}  $$\n",
        "\n",
        "\n",
        "* Random variable X = outcome of rolling a fair 6-sided die.\n",
        "\n",
        "* Sample space: 𝜒={1,2,3,4,5,6}.\n",
        "\n",
        "* PMF: p(x)=1/6 for each value (since fair die).\n",
        "\n",
        "We want:\n",
        "$$ \\mathbb{E}[X^2] = \\sum_{x=1}^{6}  x^2 \\cdot  1/6  $$\n",
        "\n",
        "$$ \\mathbb{E}[X^2] =   1/6 (1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2)  $$\n",
        "\n",
        "$$ \\mathbb{E}[X^2] =  91/6  \\approx 15.17 $$\n",
        "\n",
        "✅ Interpretation: If you roll a die many, many times, and each time square the result, the average squared outcome will approach 15.17\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1gDFZR2S1-9HyTM29qO03SYRZxMIkdwtl)\n",
        "\n"
      ],
      "metadata": {
        "id": "qNXNzjPb-5D7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Expectations for continuous distribution:\n",
        "\n",
        "while for a continuous distribution, it's found by integrating the product of each value and its probability density function (∫ xf(x)dx). Essentially, it's a weighted average of all possible values, where the weights are the probabilities.\n",
        "\n",
        "2. Continuous Random Variable\n",
        "\n",
        "When X is continuous, probabilities are described using the probability density function (PDF), $ f_X(x)$\n",
        "\n",
        "👉 Instead of a sum, we use an integral:\n",
        "\n",
        "$$ $$\n",
        "\n",
        "* g(x) = some function of X.\n",
        "\n",
        "* $ f_X(x)$ = PDF of X\n",
        "\n",
        "* The integral accumulates the weighted contributions over all real values of x.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=19L3ODIptv_RB29b4V_kIWp9beE8OUhZq)\n",
        "\n",
        "\n",
        "In the continuous case, there are infinitely many possible values (like height, time, or temperature).\n",
        "\n",
        "You can’t just “add” probabilities like in a discrete list, because each individual value has probability 0.\n",
        "\n",
        "Instead, you use an integral (continuous sum) to “accumulate” probability mass across values.\n",
        "\n",
        "👉 So the integral is the continuous analog of the sum."
      ],
      "metadata": {
        "id": "BgVBJdxPMK_k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Question\n",
        "What is the expected value of rolling a die if you earn \\\\$1 for an odd number and \\\\$2 for an even number?\n",
        "\n",
        "The expected value is calculated as\n",
        "$$ E(x) = ( 1 \\cdot \\frac{3}{6}) + (2 \\cdot \\frac{3}{6}) $$\n",
        "$$ 1.5 $$"
      ],
      "metadata": {
        "id": "eKj7fmCuM9lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Question\n",
        "Imagine playing a game where you win \\$5 with a probability of $ \\frac{1}{2} $ and lose \\$3 with a probability of $\\frac{1}{2}$. What is the expected value of the game?\n",
        "\n",
        "The expected value is calculated as\n",
        "$$ E(x) = ( 5 \\cdot \\frac{1}{2}) - (3 \\cdot \\frac{1}{2}) $$\n",
        "$$ 1 $$\n",
        "\n",
        "So, on average, you win $1 per game."
      ],
      "metadata": {
        "id": "J0jGUTMcRuj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Question\n",
        "In a lottery, you have a probability of $\\frac{1}{1000} $ to win \\$1000, otherwise you lose $1. What is the expected value of playing the lottery?\n",
        "\n",
        "The expected value is calculated as\n",
        "$$ E(x) = ( 1000 \\cdot \\frac{1}{1000}) - (1 \\cdot 1- \\frac{1}{1000}) $$\n",
        "$$ 0.001 $$\n",
        "\n",
        "So, on average, you win \\$0.001 per game."
      ],
      "metadata": {
        "id": "q_IQAHylShHl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Properties of Expectation"
      ],
      "metadata": {
        "id": "Bi4RjqsojzQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Property 1: Constants and Scaling**\n",
        "\n",
        "$ E[a] = a$ , $ E[ag(X)] = aE[g(X)] $\n",
        "\n",
        "What it means: If you take a contast (say a= 5), its expectation is just the constant itself:\n",
        "\n",
        "$$ E[5] = 5 $$\n",
        "\n",
        "Beause its not random.\n",
        "\n",
        "If you scale a random variable by a constant a:\n",
        "\n",
        "$$  E[ag(X)] = a E[g(X)] $$\n",
        "\n",
        "This is called the linearity of expectation with respect to scaling.\n",
        "\n",
        "**Example: **\n",
        "Let X = outcome of a fair die, so E[X] = 3.5 (1 x 1/6 + 2 x 1/6 + 3 x 1/6 + 4 x 1/6 + 5 x 1/6 + 6 x 1/6)\n",
        "\n",
        "Then E[2X] = 2.E[X] = 7\n"
      ],
      "metadata": {
        "id": "RbQTUfwHuQls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Property 2: Linearity of Expectation (Addition Rule)**\n",
        "\n",
        "$$ E[f(X) + g(X)] = E[f(X)] + E[g(X)] $$\n",
        "\n",
        "What it means: The expectation of a sun is the sum of expectations. This is true always (no indepedence required.)\n",
        "\n",
        "Example: Let X = die roll, Y = coin flip ( 1=Head, 0=Tail).\n",
        "\n",
        "\n",
        "\n",
        "*   E[X] = 3.5, E[Y] = 0.5\n",
        "*   Then E[X + Y] = E[X] + E[Y] = 3.5 + 0.5 = 4\n",
        "\n",
        "Even though a coin and a die are totally different experiments, expectation \"adds up\" beautifully.\n",
        "\n"
      ],
      "metadata": {
        "id": "4ggDj3yA3Btz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Variance"
      ],
      "metadata": {
        "id": "eGd1QDdL3_Kx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variance is a measure of how spread out the values of a random variable are around its mean.\n",
        "\n",
        "If the variance is small, values of the random variable cluster closely around the mean.\n",
        "\n",
        "If the variance is large, values are more spread out.\n",
        "\n",
        "Formally, for a random variable X:\n",
        "\n",
        "$$ Var(X) = E[(X - E[X])^2 $$\n",
        "\n",
        "## Intuition\n",
        "*  The mean E[X] tell us the average outcome.\n",
        "*  Variance tells us how far, on average, outcomes deviate from that average.\n",
        "*  Spaquring (X - E[X]):\n",
        "** Makes Deviations positive\n",
        "** Penalizes larger deviations more heavily\n",
        "\n",
        "So Vairance captures the average distance from the mean.\n",
        "\n",
        "## Alternative formula\n",
        "\n",
        "There's a useful shortcut:\n",
        "\n",
        "$$ Var(X) = E[X^2] - (E[X])^2 $$\n",
        "\n",
        "If you Var(X) given - you can caucluate the others from other equation.\n",
        "\n",
        "General Identity:\n",
        "For any random variable X, there's a very useful identity (which we computed in above section)\n",
        "\n",
        "$$ E[(a_i - b_i)^2] = Var(a_i - b_i) + (E[a_i - b_i])^2$$\n",
        "\n",
        "Or\n",
        "\n"
      ],
      "metadata": {
        "id": "DUSejPdH4CB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1C9z-KeSFHTW9yrZSOC4EUgvDeSre_gJt)\n"
      ],
      "metadata": {
        "id": "To3uURMa7X4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Question\n",
        "\n",
        "Question 1: Define a random variable (r.v.) that assigns a value 1 if the flip of a coin comes out Heads, and 0 if it comes out Tails. Input the mean and variance for the r.v. defined as the sum of two fair coins flipped simultaneously (Heads = 1 and Tails = 0).\n",
        "\n",
        "\n",
        "Let $X_1$ = 1st Flip\n",
        "\n",
        "and $X_2$ = 2nd Flip\n",
        "\n",
        "where X₁ and X₂ are independent Bernoulli(1/2) random variables. $ Y \\in {0,1,2} $\n",
        "\n",
        "**Step # 4: Find Mean of the Sum**\n",
        "Using the linearity of expectation: $ E[Y] = E[X_1 + X_2] $\n",
        "$ = E[X_1] + E[X_2] $\n",
        "$ = 1/2 + 1/2 = 1 $\n",
        "\n",
        "**Step # 5: Find the Variance of the Sum**\n",
        "\n",
        "Since X₁ and X₂ are independent, we can use:\n",
        "Var(Y) = Var(X₁ + X₂) = Var(X₁) + Var(X₂) = 1/4 + 1/4 = 1/2\n",
        "\n",
        "\n",
        "**Key Concepts Used:**\n",
        "\n",
        "Linearity of Expectation: E[X + Y] = E[X] + E[Y] (always true)\n",
        "Independence for Variance: If X and Y are independent, then Var(X + Y) = Var(X) + Var(Y)\n",
        "*Bernoulli Distribution:* A single trial with two outcomes (success/failure)\n",
        "\n",
        "**Alternative Verification:**\n",
        "You can verify this by listing all possible outcomes:\n",
        "\n",
        "(T,T): Y = 0, P = 1/4\n",
        "(T,H) or (H,T): Y = 1, P = 1/2\n",
        "(H,H): Y = 2, P = 1/4\n",
        "\n",
        "E[Y] = 0×(1/4) + 1×(1/2) + 2×(1/4) = 1 ✓\n",
        "\n",
        "Var(Y) = E[Y²] - (E[Y])² = (0²×1/4 + 1²×1/2 + 2²×1/4) - 1² = 1.5 - 1 = 0.5 ✓"
      ],
      "metadata": {
        "id": "w3N0VjgU8zdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Question\n",
        "\n",
        "Input the mean and variance (separated by comma and space) of the random variable associated with a fair die with 6 faces (answer non-integer numbers as irreducible fractions).\n",
        "\n",
        "Step # 1:\n",
        "\n",
        "Let X = outcome of a fair die roll\n",
        "* Possible values : {1,2,3,4,5,6}\n",
        "* Each equally likely P(X = x) = 1/6\n",
        "\n",
        "Step # 2:\n",
        "\n",
        "$$ E[X] = \\sum^{6}_{x=1} \\cdot P(X=x) $$\n",
        "$$ (1+2+3+4+5+6) \\cdot 1/6 $$\n",
        "$$ E[X] = 21/6 = 7/2 $$\n",
        "\n",
        "Step # 3: Formula for Variance:\n",
        "\n",
        "$$ Var(X) = E[X^2] - (E[X])^2 $$\n",
        "\n",
        "First Compute $E[X^2]$ :\n",
        "\n",
        "$$ E[X^2] = \\sum^{6}_{x=1} x^2 \\cdot P(X=x) = \\frac{1}{6} \\sum^{6}_{x=1} x^2 $$\n",
        "\n",
        "\n",
        "$$ x^2 = 1^2 + 2^2 + 3^2 + 4^2 + 5^2 + 6^2 = 91 $$\n",
        "\n",
        "$$ E[X^2] = 91 / 6 $$\n",
        "\n",
        "Now Variance:\n",
        "\n",
        "$$ Var(X) = 91/6  - (7/2)^2 $$\n",
        "\n",
        "$\n",
        "$ = 91/6 - 49/ 4 = 35/12 $$\n"
      ],
      "metadata": {
        "id": "5mK86xhrFeCX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability Distribution:\n",
        "\n",
        "\n",
        "\n",
        "*   Normal Distribution\n",
        "*   Uniform Ditribution\n",
        "*   Binomial Distribution\n",
        "*   Poisson Distribution\n",
        "*   Triangular Distribution\n",
        "*   Weibull Distribution\n",
        "*   Bernoulli Distribution\n",
        "\n",
        "\n",
        "Reference: https://www.youtube.com/watch?v=3VylC_mIAjE\n"
      ],
      "metadata": {
        "id": "oJfanSUWQLzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- T-Distribution\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1mnaZ51NrxCJjnDlt83TsD4taFMEZvj3d)\n"
      ],
      "metadata": {
        "id": "HnF1OUAfUb0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3- Bernoulli Distribution - Common Discrete R.V\n",
        "\n",
        "Special Case of Binomial Distribution. Instead of considering all the outcomes across X axis, we are just considering the 2 outcomes. True or False or (Yes or No)\n",
        "\n",
        "For example: How likely we were to roll Six, Yes or No kind of question falls in Bernoulli Distribution.\n",
        "If we did this enough time, we should end up 1 out of eveyr 6 times.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1A3Cw_3usXyDKJQ54DZJm-sdpIHBIA2LH)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1TaPHzwFIWbHBg-CPwb3Ed2eahSEwGeOn)\n",
        "\n",
        "\n",
        "**Khan Academy Resource**\n",
        "Reference Video: https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/binomial-mean-standard-dev-formulas/v/mean-and-variance-of-bernoulli-distribution-example\n",
        "\n",
        "\n",
        "**Expectations and Variance of a Bernoulli Distribution**\n",
        "\n",
        "mean:\n",
        "$$ \\mathbb{E}[X] = 1 \\cdot p + 0 \\cdot (1 - p) = p $$\n",
        "variance:\n",
        "$$  p(1-p) $$\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1zFf0ULftcpaqwqVfg6lc-UGeVHuE4dG1)\n"
      ],
      "metadata": {
        "id": "28Qd1CM2V-L7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Binomial Distribution - Common Discrete R.V\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1CAU1PyFfHfFnt9xIvwBbo4yPxMuVKgug)\n",
        "\n",
        "\n",
        "🔹 What the Graph Shows\n",
        "\n",
        "This is a Binomial Distribution with:\n",
        "\n",
        "Number of trials  𝑛 = 10 (flipping 10 coins).\n",
        "\n",
        "Probability of success (Heads) : p=0.5.\n",
        "\n",
        "Random variable  X = number of Heads observed.\n",
        "\n",
        "The x-axis = number of heads (0 through 10).\n",
        "The y-axis = probability of that outcome.\n",
        "\n",
        "\n",
        "🔹**How to Interpret the Bars**\n",
        "\n",
        "*Example*: The bar at X=5 shows ~24.6%.\n",
        "\n",
        "→ If you flip 10 coins, the most likely outcome is 5 heads (about 25% of the time).\n",
        "\n",
        "*Example*: The bar at X=0 shows 0.1%.\n",
        "→ This does not mean “0 heads OR 0 tails” in a single flip.\n",
        "\n",
        "→ It means: out of 10 flips, the chance of getting all Tails (0 Heads) is 0.1%.\n",
        "\n",
        "Similarly, the bar at  X=10 = 0.1%.\n",
        "→ The chance of getting all Heads in 10 flips is also 0.1%.\n",
        "\n",
        "\n",
        "**Binomial Distribution Basics:**\n",
        "\n",
        "* X ~ Bin(n,p) means X follows a binomial distribution\n",
        "* n = number of independent trials\n",
        "* p = probability of success on each trial\n",
        "* X counts the total number of successes in n trials\n",
        "\n",
        "**What is Variance:**\n",
        "Variance measures how spread out the values are from the mean. For any random variable:\n",
        "\n",
        "* Variance = E[(X - μ)²] where μ is the mean\n",
        "* It tells us about the \"scatter\" or variability of the distribution\n",
        "\n",
        "**Key Properties of Binomial Distribution:**\n",
        "\n",
        "Each trial has only two outcomes: success (probability p) or failure (probability 1-p)\n",
        "Trials are independent\n",
        "The probability p remains constant across all trials.\n",
        "\n",
        "**Conceptual Thinking for Binomial Variance:**\n",
        "\n",
        "* If p is close to 0 or 1: Most outcomes will be very predictable (low variance)\n",
        "* If p is around 0.5: Maximum uncertainty in each trial (higher variance)\n",
        "* More trials (larger n): Generally increases the total variability\n",
        "* The variance depends on both the number of trials AND the uncertainty in each trial\n",
        "\n",
        "** Mathematical Foundation:**\n",
        "Since binomial is the sum of n independent Bernoulli trials, and variances of independent random variables add up, the binomial variance relates to:\n",
        "\n",
        "* The variance of a single Bernoulli trial\n",
        "* Multiplied by the number of trials\n",
        "\n",
        "\n",
        "$$ px(h) = (^n_h) P^h (1-p) ^{(n-h)} $$"
      ],
      "metadata": {
        "id": "wxWhsVdsUtSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-  Uniform Distribution - Common Continuous Random Variable (R.V)\n",
        "\n",
        "What is a Uniform Distribution?\n",
        "\n",
        "* A uniform distribution is a probability distribution where every outcome in a given interval is equally likely.\n",
        "\n",
        "* If a random variable X follows a uniform distribution between a and b, we write:\n",
        "$$ X \\backsim Unif(a,b) $$\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Ipt9jMwGOKwsjSILAf7RHDtWGDsWQCyI)\n",
        "\n",
        "Mean for Uniform Distribution:\n",
        "\n",
        "Mean:\n",
        "$$ E[X] = \\frac{a+b}{2} $$\n",
        "\n",
        "Variance:\n",
        "$$ Var(X) = \\frac{(b-a)^2}{12} $$\n"
      ],
      "metadata": {
        "id": "zZPSb1znYQVn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Empirical Mean\n",
        "![](https://drive.google.com/uc?export=view&id=1HWGSm3jRZKUgo1-W5fJC7BGqU6rxicf4)\n"
      ],
      "metadata": {
        "id": "I7lBlPmDfdvA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OYDPV7CJimXK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        " Input the mean and variance of a random real number chosen uniformly at random from the range [-2, 2].\n",
        "\n",
        " ## Answer\n",
        "for X $ \\approx Uniform(a,b): $\n",
        "\n",
        "Mean can be calculated as\n",
        "The mean $\\mu $ of a uniform distribution is calculated using the formula\n",
        "\n",
        "$ \\frac{a+b}{2}\\$\n",
        "\n",
        "$$ E[X] = \\frac{a+b}{2} $$\n",
        "\n",
        "Variance:\n",
        "The variance $ (\\sigma ^{2}) $ of a uniform distribution is calculated using the formula $ (\\sigma ^{2}=\\frac{(b-a)^{2}}{12}) $.\n",
        "\n",
        "$$ Var(X) = \\frac{(b - a)^2}{12}$$\n",
        "\n",
        "Using formula: Mean is $ E[X] = \\frac{-1 + 2}{2} = 0/2 = 0 $\n",
        "\n",
        "and Variance is\n",
        "\n",
        "$$ Var(X) = \\frac{(2-(-2))^2}{12} = \\frac{4^2}{12} = \\frac{16}{12}= \\frac{4}{3}  $$\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1hNj8wS1hrcEKlFkLiXgpuk5Mhollwok8)"
      ],
      "metadata": {
        "id": "i9orb7-IkIFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "\n",
        "Consider N indepdent random variables $X_1, X_2, ...., X_N $ with identical means and variances equal to $\\mu $ and $ \\sigma^2 $, respectively.\n",
        "\n",
        "What is the mean of the empirical mean, defines as\n",
        " ## Answer\n",
        " We have N independent and identically distributed random variables:\n",
        "\n",
        " $$ X_1, X_2, ...., X_N $$\n",
        "\n",
        " with\n",
        "\n",
        " $$ E[X_i] = \\mu $$\n",
        "\n",
        " $$ Var(X_i) = \\sigma^2 $$\n",
        "\n",
        "\n",
        "\n",
        " Question is: what is the mean(expectation) of the empirical mean (empirical mean is defined as\n",
        "  $$ \\bar{X} = \\frac{1}{N} \\sum^{N}_{i=1} X_i $$\n",
        "\n",
        "  so we basically want $ E[\\bar{X}] $\n",
        "\n",
        "  We know from Use of Linearity of Expectations:\n",
        "\n",
        "  $$ E[aX + bY] = aE[X] + bE[Y]  $$\n",
        "\n",
        "  replacing the $ \\bar{X} in the above equation:\n",
        "\n",
        "  $$ E[-] = aE[-] $$\n",
        "\n",
        "  $$ E[\\bar{X}] = E[ \\frac{1}{N} \\sum^{N}_{i=1} X_i] $$\n",
        "  $$ E[\\bar{X}] = \\frac{1}{N} E[ \\sum^{N}_{i=1} X_i] $$\n",
        "  $$ E[\\bar{X}] = \\frac{1}{N} \\sum^{N}_{i=1} E[X_i] $$\n",
        "\n",
        "We can \"pull out\" the summation from inside the expectation because:The linearity of expectation states that: E[X + Y] = E[X] + E[Y]\n",
        "\n",
        "since each $E[X_i] = \\mu $\n",
        "$$ E[\\bar{X}] = \\frac{1}{N} N\\mu $$\n",
        "\n",
        "N cancels out and we are left with\n",
        "$$ E[\\bar{X}] = \\mu $$\n",
        "\n",
        "Whenever you see a question like this:\n",
        "\n",
        "Recognize the structure: empirical mean = sum of r.v.s divided by N.\n",
        "\n",
        "1.   Recognize the structure: empirical mean = sum of r.v.s divided by N.\n",
        "2.   Apply linearity of expectation: you can pull constants out and split sums.\n",
        "3.   Substitute known expectations: if each 𝐸[𝑋𝑖]=𝜇, then the average also has expectation 𝜇\n",
        "\n",
        "👉 This is why the sample mean is an unbiased estimator of the population mean.\n",
        "\n",
        "Population mean (𝜇) = the true mean of the entire distribution (theoretical).\n",
        "\n",
        "Empirical mean (𝑋ˉ) = the mean of your finite observed sample.\n"
      ],
      "metadata": {
        "id": "W0BL2V7Yk23k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistical Learning\n"
      ],
      "metadata": {
        "id": "M6KLl63D-8y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Sections 2.1 and 2.2 in Chapter 2 from the book Introduction to Statistical Learning\n",
        "\n",
        "This module introduces the standard theoretical framework used to analyze statistical learning problems. We start by covering the concept of regression function and the need for parametric models to estimate it due to the curse of dimensionality. We continue by presenting tools to assess the quality of a parametric model and discuss the Bias-Variance tradeoff as a theoretical framework to understand overfitting and optimal model flexibility. Finally, we will continue our introduction to the Python programming language.\n",
        "\n",
        "**Learning Objectives**\n",
        "\n",
        "* Understand the theoretical framework to analyze statistical learning problems and the regression function.\n",
        "* Understand the curse of dimensionality.\n",
        "* Understand the parametric models and the bias-variance tradeoff.\n",
        "* Evaluate model quality and optimal model flexibility.\n",
        "* Create program in Python using Jupyter notebooks."
      ],
      "metadata": {
        "id": "TywE0_yS_YSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to motivate our study of statistical learning, we begin with a simple example. Suppose that we are statistical consultants hired by a client to provide advice on how to improve sales of a particular product. The Advertising data set consists of the sales of that product in 200 different markets, along with advertising budgets for the product in each of those markets for three different media: TV, radio, and newspaper. The data are displayed in Figure 2.1. It is not possible for our client to directly increase sales of the product. On the other hand, they can control the advertising expenditure in each of the three media. Therefore, if we determine that there is an association between advertising and sales, then we can instruct our client to adjust advertising budgets, thereby indirectly increasing sales. In other words, our goal is to develop an accurate model that can be used to predict sales on the basis of the three media budgets.\n",
        "In this setting, the advertising budgets are input variables while sales is an output variable. The input variables are typically denoted using the symbol X, with a subscript to distinguish them. So X1 might be the TV budget, X2 the radio budget, and X3 the newspaper budget. The inputs go by different names, such as predictors, independent variables, features, or sometimes just variables. The output variable—in this case, sales—is often called the response or dependent variable, and is typically denoted using the symbol Y . Throughout this book, we will use all of these terms interchangeably.\n",
        "\n",
        "More generally, suppose that we observe a quantitative response Y and p different predictors, X1, X2, . . . , Xp. We assume that there is some relationship between Y and X = (X1, X2, ..., Xp), which can be written in the very general form\n",
        "\n",
        "$ Y =f(X)+ε $\n",
        "\n",
        "Here f is some fixed but unknown function of  𝑋1,...,𝑋𝑝  , and ε is a random error term, which is independent of X and has mean zero. In this formulation, f represents the systematic information that X provides about Y .\n",
        "\n",
        "We assume that X is a vector of random variables and Y is scalar variable.\n"
      ],
      "metadata": {
        "id": "ABIlZPuiUxjQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In essence, statistical learning refers to a set of approaches for estimating f . In this chapter we outline some of the key theoretical concepts that arise in estimating f, as well as tools for evaluating the estimates obtained.\n"
      ],
      "metadata": {
        "id": "ECi-qgUeWOjj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prediction**\n",
        "In many situations, a set of inputs X are readily available, but the output Y cannot be easily obtained. In this setting, since the error term averages to zero, we can predict Y using\n",
        "\n",
        "$ \\hat{Y} = f(X) $\n",
        "\n",
        "where $ \\hat{f} $  represents our estimate for f , and Yˆ represents the resulting prediction for Y . In this setting, fˆ is often treated as a black box, in the sense that one is not typically concerned with the exact form of f, provided that it yields accurate predictions for Y ."
      ],
      "metadata": {
        "id": "zzW-jBEhXc83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 1. The idea of 𝑓**\n",
        "\n",
        "In supervised learning, we assume there is some true relationship between inputs 𝑋 and output 𝑌\n",
        "$ 𝑌 = 𝑓(𝑋) + 𝜀 $\n",
        "\n",
        "𝑓 = the true (but unknown) function that links  𝑋 and 𝑌.\n",
        "\n",
        "𝜀 = random error (noise), usually assumed to average to 0.\n",
        "\n",
        "👉 Example: If 𝑋 = house size and 𝑌 = price, then\n",
        "\n",
        "𝑓( 𝑋 ) is the true “law” of how size affects price.\n"
      ],
      "metadata": {
        "id": "babGi6gDZUjh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 2. The role of $ \\hat{f}(𝑋) $**\n",
        "\n",
        "Since we don’t know the true 𝑓, we estimate it from data. That estimate is written as:\n",
        "\n",
        "$ \\hat{f}(𝑋) $  \n",
        "* The hat symbol means “estimate of.”\n",
        "\n",
        "* $ \\hat{f}$ could come from linear regression, decision trees, neural networks, etc.\n",
        "\n",
        "👉 Example: If you fit a line through data points (linear regression), that fitted line is your 𝑓^\n",
        "\n",
        "**🔹 3. Using $ \\hat{f}$ to make predictions**\n",
        "\n",
        "Once we have  $ \\hat{f}$ , we can predict $ \\hat{Y}$.\n",
        "\n",
        "$ \\hat{Y}$ = the predicted outcome (not the true Y, just our model’s guess).\n",
        "\n",
        "In practice, $ \\hat{f}$  is often treated as a black box — we may not know or care about its exact formula, as long as it predicts well."
      ],
      "metadata": {
        "id": "kJbcBm0IZ2XK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 4. Prediction setup**\n",
        "\n",
        "We want to predict Y from inputs X.\n",
        "\n",
        "True relationship:\n",
        "$$ Y = f(X) + \\varepsilon $$\n",
        "where $ \\varepsilon $ = random noise (can't be predicted).\n",
        "* Our model uses an estimate $ \\hat{f} $:\n",
        "$$ \\hat{Y} = \\hat{f}(X) $$\n",
        "So the prediction error is:\n",
        "\n",
        "$$ Y - \\hat{Y} = f(X) + \\varepsilon - \\hat{f}(X) $$\n"
      ],
      "metadata": {
        "id": "3CWbnZaxdF_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 5. Two sources of error**\n",
        "\n",
        "The text introduces two categories of error:\n",
        "\n",
        "**Reducible error**\n",
        "\n",
        "* This comes from the fact that our estimate $ \\hat{f}$ is not exactly the true f.\n",
        "* If we choose better algorithms, more data, or tune parameters, we can reduce this gap.\n",
        "\n",
        "Formally:\n",
        "$$ [f(X) - \\hat(X)]^2$$\n",
        "\n",
        "**Irreducible error**\n",
        "\n",
        "This comes from the noise term $ \\varepsilon $\n",
        "\n",
        "Even if we knew the true f exactly, we cannot predict ε because it represents random variation (e.g., unmeasured factors, randomness in the world).\n",
        "\n",
        "Formally: $ Var(\\varepsilon)$  \n",
        "\n",
        "**🔹 6. Why irreducible error > 0?**\n",
        "\n",
        "There are always factors affecting Y that are not in X.\n",
        "\n",
        "**Example**: A patient’s reaction to a drug might vary due to stress, sleep, manufacturing variation in the pill, etc. These things are unpredictable with the data we have.\n",
        "\n",
        "So no matter how good your model is, there will always be some error left."
      ],
      "metadata": {
        "id": "wAq1dcHeeF26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 7. What are we trying to measure?**\n",
        "\n",
        "When we build a prediction model, we want to measure:\n",
        "\n",
        "How far off are our predictions $\\hat{Y}$ from the true values Y?\n",
        "\n",
        "A natural measure is the squared prediction error:\n",
        "\n",
        "$$ (Y - \\hat{Y})^2 $$\n",
        "\n",
        "Squaring is useful because:\n",
        "* it makes errors positive (no cancellation of under/over predictions).\n",
        "* it penalizes large errors more heavily.\n"
      ],
      "metadata": {
        "id": "jiOpviy8fu7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 8. Why expectation?**\n",
        "\n",
        "One prediction error $ (Y - \\hat{Y})^2 $  depends on one data point. But we want a general measure of average error across possible data. So we take the expected value (the average over the data distribution):\n",
        "\n",
        "$$ E [(Y - \\hat{Y} )^2 ] $$\n",
        "\n",
        "This is called the expected prediction error or mean squared error (MSE)"
      ],
      "metadata": {
        "id": "mXu7kn_JgQFq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J2_ZFnVcgKp7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**🔹 8. Substituting the data-generating process**\n",
        "\n",
        "We know from the model:\n",
        "\n",
        "$$ Y = f(X) + \\varepsilon $$\n",
        "\n",
        "and our prediction is:\n",
        "\n",
        "$$ \\hat{Y} = \\hat{f}(X)  $$\n",
        "\n",
        "So the error becomes:\n",
        "\n",
        "$$ Y - \\hat{Y} = f(X) + \\varepsilon -  \\hat{f}(X)  $$\n",
        "\n",
        "Squaring\n",
        "$$ ( Y - \\hat{Y}) ^2  = ( f(X) + \\varepsilon -  \\hat{f}(X) ) ^2  $$\n",
        "\n",
        "**🔹 9. Taking expectation**\n",
        "$$ E [( Y - \\hat{Y}) ^2]  = E[( f(X) + \\varepsilon -  \\hat{f}(X) ) ^2 ] $$\n",
        "$$ = (f(X) -  \\hat{f}(X) )^2 + 2(\\hat{f}(X) -  \\hat{f}(X)) \\varepsilon + \\varepsilon^2  $$\n",
        "\n",
        "\n",
        "$$ = (f(X) -  \\hat{f}(X) )^2 + 2(\\hat{f}(X) -  \\hat{f}(X))E[\\varepsilon] + E[\\varepsilon^2]  $$\n",
        "\n",
        "* The middle term vanishes because $ E[ɛ]$ = 0 . This is an assumption in most regression/statistical models: the noise has zero mean (no bias, just random fluctuation) and some finite variance. and the last term: $ E[ɛ^2] $ = Var(ɛ)\n",
        "\n",
        " $$ E [( Y - \\hat{Y}) ^2]  = (f(X) -  \\hat{f}(X) )^2  + Var(\\varepsilon)  $$\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1ckPYFbGfuZD3t_l40E_QxE-bYwLLKZpV)"
      ],
      "metadata": {
        "id": "RMftt_VdhWO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1pyCYalpeNV6Tl2AkuDnHaCvblGmc8vbx)"
      ],
      "metadata": {
        "id": "nXrnmwPdm-S8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression function\n",
        "\n",
        "Using tools from statistics, one can prove that the regression function is equal to this conditional expectation,\n",
        "\n",
        "$$ f(x) = \\mathbb{E} [Y | X = x] $$\n",
        "\n",
        "essentially the expectation of Y given X.\n",
        "\n",
        "If we have access to the conditional pdf f of Y given X, we could compute this expectation $ f(x) = \\mathbb{E}[Y | X = x] $ by using the explicit formula that we introduced before. Essentially, we need to compute the integral of Y times\n",
        "$f_{Y|X} $ and then integration goes over Y.\n",
        "\n",
        "Notice that the result of the integral is a function of $ \\textbf{x}$ alone.\n",
        "\n",
        "$$ f(x) = \\mathbb{E} [Y|X = x] = \\int_{y=+\\infty. }^{y=-\\infty.} y \\; f_{Y|X} \\; (y|\\textbf{x}) dy $$\n",
        "\n",
        "\n",
        "\n",
        "## 🔹 1. Conditional expectation definition\n",
        "The regression function is defined as:\n",
        "\n",
        "$$ f(x) = E[Y | X = x] $$\n",
        "\n",
        "This means:\n",
        "\n",
        "For a given input value 𝑥, the “best guess” of 𝑌 is the conditional expectation of 𝑌 given 𝑋=𝑥\n",
        "\n",
        "In words: if you know the input\n",
        "𝑋=𝑥, what’s the average value of 𝑌 you should expect?\n",
        "\n",
        "👉 Example: If 𝑋 = study hours and 𝑌 = exam score, then  𝑓(5) means: “the expected exam score if someone studies 5 hours.”\n",
        "\n",
        "## 🔹 2. Expressed with conditional density\n",
        "If the conditional density of Y given X = x if $f_{Y|X} (y|x) $, then:\n",
        "$$ f(x) = \\mathbb{E} [Y|X = x] = \\int_{y=+\\infty. }^{y=-\\infty.} y \\; f_{Y|X} \\; (y|\\textbf{x}) dy $$\n",
        "\n",
        "This integral just says:\n",
        "* Where $ f_{Y|X} \\; (y|\\textbf{x}) $ is the conditional probablity density function of Y given X = $\\textbf{x} $\n",
        "* Take all possible values of Y.\n",
        "* Weight them by how likely they are (the conditional density).\n",
        "* That gives the average = the regression function\n",
        "*\n",
        "\n",
        "## 🔹 3. Why it depends only on x\n",
        "\n",
        "Notice that once you itnegrate over all possible y, the result is just a function of the input x.\n",
        "\n",
        "That's why f(x) is called the regression function of Y on X.\n",
        "\n",
        "## 🔹 4. Optimization characterization\n",
        "\n",
        "Another way to define the regression function or The regression function can also be viewed as the solution to an optimization problem:\n",
        "\n",
        "$$ f(.) = \\underset{g(.)}{\\mathrm{argmin}} \\mathbb{E} [(Y - g(X))^2] $$\n",
        "\n",
        "This says:\n",
        "\n",
        "* Imagine trying to approximate Y with some function of X, call it g(X).\n",
        "* measure how good your guess is using mean squared error (MSE):\n",
        "\n",
        "Here, one seeks the function g that minimizes the expected squared error (mean squared error, MSE) between the actual value Y and prediction g(X).\n",
        "\n",
        "Key Insights:\n",
        "* The regression function depends only on x and not directly on y.\n",
        "* Although the probablity density function $ F_{Y|X} $ is usually unknown in practice, this framework justifies why learning algorithms try to estimate conditional expectation.\n",
        "\n",
        "\n",
        "**In Summary: **\n",
        "\n",
        "The regression function f(x) is the conditional mean of $ f_{Y|X}$, which can be computed if the ocnditional density is known, and proves that it minimizes the mean squared error among all possible funcrions to predict Y from X.\n",
        "* The regression function is not necessarily a line or polynomial; its just the conditional expectation E[Y|X = x].\n",
        "* All regression methods (Linear, logistic, neural nets, etc.) are ways of trying to estimate $\\hat{f}(x)$, since the true f(x) is usually unknown.\n"
      ],
      "metadata": {
        "id": "ZbO7rIfqsja7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "Let's work through a concerte example step by step, so you can see how the regression function. compute $ \\mathbb{E}[Y|X=x]$ for a simple case like\n",
        "\n",
        "$$ Y = 2X + \\varepsilon $$\n",
        "\n",
        "Where\n",
        "* X is the input,\n",
        "* $ \\varepsilon $ N(0,1) (noise, normally distributed with mean 0 and variance 1\n",
        "\n",
        "By definition: $ f(x) = \\mathbb{E}[Y | X = x] $\n",
        "\n",
        "Substitute the model for Y:\n",
        "$$ f(x) = \\mathbb{E} [2X + ɛ | X = x] $$\n",
        "Using linearity of expectation:\n",
        "$$ f(x) = \\mathbb{E}[2X | X = x] + \\mathbb{E}[ɛ | X =x]  $$\n",
        "\n",
        "first term: $ \\mathbb{E}[2X | X = x] $ = 2x. (That’s why it is not\n",
        "E[2x] (because 2x isn’t random anymore, it’s just a number). The expectation of a constant is the constant itself.)\n",
        "\n",
        "\n",
        "Second term: Since $ ɛ $ is indepedent of X and has mean 0,\n",
        "$$ f(x) = 2x + 0  $$"
      ],
      "metadata": {
        "id": "lzP6-dSac5CJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question\n",
        "\n",
        "In Practice, we do not know explicitly the conditional PDF $ f_{Y|X} $; however, we can sample data points from the additive model. Hence,\n",
        "Given a dataset D = $ {(x_1,y_1), . . . . (x_N,y_N)}$  where $(x_i, y_i)$ are random samples drawn independetly from the additive model (notice that $x_i $ ($x_{i1}, ..... x_{ip})^T $ $ \\in \\mathbb{R}^P $ is a p-dimensional vector).\n",
        "\n",
        "Find as estimate of the regression function f. We will denote our estimate by $ \\hat{f}$.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1dXlhmJ3e7q1YR8ZYpBFSHVxcHdWC9EmS)\n",
        "\n",
        "\n",
        "$ y_i = f(x_i) + ɛ_i $\n",
        "where $ \\mathbb{E}[ɛ_i] = 0 $\n",
        "\n",
        "Using approximate f(x) using the empirical conditional mean, i.e..\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1dqj0Hx6KCb2OIbaSd0WimsV1KYoMOSY9)\n",
        "\n"
      ],
      "metadata": {
        "id": "I4xtQyGfi0I6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1QL8yXDEUZ9uM0lXqK1cVCTL_idQJKUwY)\n"
      ],
      "metadata": {
        "id": "s_8bamGCo5BJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1JfM2g62pa12JEk8vWWpY1tAEnEDfh1-1)"
      ],
      "metadata": {
        "id": "rEra8nFHpBoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1llya1EeAhspdP2f6u3gH-BaFWfNEDveI)"
      ],
      "metadata": {
        "id": "GD-dRbX_pxHE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "312ZNdyXo7Tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://canvas.upenn.edu/courses/1878459/pages/%3E-regression-function-11-36?module_item_id=34205529\n",
        "window in technique\n"
      ],
      "metadata": {
        "id": "Luxfo3n_rNU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1JlIHDVcghrmAYEqfIUpLqUrT-l4ZtR_W)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1NsTZtQgUXcjO2vi1MnplCie-Ijs6YdDx)\n",
        "\n"
      ],
      "metadata": {
        "id": "mrg9yx_6xTPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iU0uSSr6qN2s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Local Averaging\n",
        "The slide is explaining local averaging as a practical way to approximate the regression function f(x) when you only have sample data.\n",
        "\n",
        "In theory, we said:\n",
        "\n",
        "$$ f(x) = E[Y|X =x]  $$\n",
        "\n",
        "In practice, we don’t know the true distribution, so we approximate using empirical averages of nearby data points.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=123HbKN5_3Nk_cirf0qxUYGrJlgUdQi5m)\n",
        "\n",
        "\n",
        "## 🔹 Step-by-step process (illustrated in the figure)\n",
        "\n",
        "\n",
        "1.   Pick a point of interest — say, 𝑥=2\n",
        "2.   choose a window around it - e.g. interaval [2 - r, 2+r]\n",
        "3.   Collect all data points inside that windown\n",
        "4.   Average their y-values (outputs)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1gV8xIEf1j--XpybenBsb7CBqMEpCSH0h)\n",
        "\n",
        "Where $D_(2,r)$ is the set of all dat apoints with inputs in [2-r, 2+r].\n",
        "\n",
        "5.   That average gives an estimate of f(2). Graphically, this is a point somewhere in the middle of the cloud of y-values around x = 2.\n",
        "6.   Repeat across all x by sliding the window, and you trace out a curve that runs throuhg the middle of the scatterplt.\n",
        "\n",
        "That curve is your estimate of the regression function.\n",
        "\n"
      ],
      "metadata": {
        "id": "2Rx_nx7dkbyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example: Predicting House Prices with 1 feature\n",
        "\n",
        "Suppose you want to estimate the price of a house(Y) based on some features (X).\n",
        "\n",
        "**Case # 1: 1D (Only one feature)**\n",
        "Let's say X = house size (sqft)\n",
        "\n",
        "\n",
        "\n",
        "*   You have a dataset of 500 houses: sizes vs. prices.\n",
        "*   if you want to estimate the price of a 2,000 sqft house, you can do local averaging:\n",
        "- Take all houses with sizes between 1,900 and 2,100 sqft, Average their prices. That Average is your estimate of f(2000).\n",
        "\n",
        "Works great! You have lots of nearby examples.\n",
        "\n",
        "\n",
        "Scatterplot: each blue dot = one house.\n",
        "\n",
        "$x_i$ = house size (sqft)\n",
        "$y_i$ = sale price\n",
        "\n",
        "Local Averaging at one point\n",
        "\n",
        "Suppose we want to estimate the price for a 2,000 sqft house:\n",
        "1. Pick up a window around 2000 (say, between 1,900 and 2,100 sqft).\n",
        "2. Collect all houses in that range (the neighbors\").\n",
        "3. Compute the average price of those neighbors.\n",
        "4. Plot that average as a single point (green dot) at x = 2000.\n",
        "\n",
        "thats $\\hat{f}$(2000), the estimated regression function at 2000 sqft.\n",
        "\n",
        "\n",
        "Slide the Winde:\n",
        "Now repeat:\n",
        "1. Move the window to 1,000 sqft, average prices there.\n",
        "2. Move to 1,500 sqft, average prices there.\n",
        "3. Move to 2,500 sqft, and so on.\n",
        "at each location, plot the average.\n",
        "\n",
        "Connect the averages:\n",
        "If you connect all those averages smoothly, you get the green line - as estimate of the true regression function f(x) = E[Y|X =x].\n",
        "\n",
        "It passes through the middle of the cloud pf blue points.\n",
        "\n",
        "It's not a perfect line, because it adapts to the local shape of the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jqk7ForGqRdn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Explanation:\n",
        "The set is $S_1$ = [-1,1] $ \\subset \\mathbb{R}^1$\n",
        "\n",
        "there are N points uniformly at random into this interval.\n",
        "\n",
        "Pick some location $ x \\in S_1 $ neighborhood around it:\n",
        "\n",
        "$$ D_r (x) = [x-t, x+r] $$\n",
        "in 1D, the length og an interval [a,b] is simply:\n",
        "Length = b-a\n",
        "\n",
        "Length(D_r(x)) = (x+r) - (x-r)\n",
        "= 2r\n",
        "\n",
        "# Density of points in 1D\n",
        "* Total interval length = 1 - (-1) = 2\n",
        "if the interval of length has N points uniformly distributed, then the expected number of points per unit length is:\n",
        "\n",
        "$$ Density = \\frac{N}{ total \\; length } = \\frac{N}{2} $$\n",
        "\n",
        "So, the expected number of points inside that window is:\n",
        "\n",
        "Expected number in window = Density × Length of window\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1S7AQHCFOuktIMDOAyC6tHpUjXKh3vR9n)\n",
        "\n"
      ],
      "metadata": {
        "id": "OLUiw9boIA7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1ByAzzCWzkQJcOF6kHxbBr0czKCokaNI8)"
      ],
      "metadata": {
        "id": "8aeSFx67Lc8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Curse of Dimensionality\n",
        "\n",
        "This approach works fine when = 1 (only one input variable).\n",
        "But when you have many input variables (p>1), something called the curse of dimensionality kicks in.\n",
        "\n",
        "*   Data points become sparse in high-dimensional space.\n",
        "*   Your local window may contain very few or no neighbors, even with a large dataset.\n",
        "*   Local averaging becomes unrealible.\n",
        "\n",
        "So while local averaging is a nice intuitive method in 1D, in higher dimensions we need more sophisticated statistical learning techniques (like linear regression, kernel methods, decision trees, neural nets, etc.)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rLoMCM2twGA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "uaL4NzXxys1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case 2: High-D (Many features)\n",
        "\n",
        "\n",
        "*   $X_1$ = house size,\n",
        "*   $X_2$ = number of bedrooms\n",
        "*   $X_3$ = lot size,\n",
        "*   $X_4$ = year built\n",
        "*   $X_5$ = distance to city center.\n",
        "\n",
        "Thats 5D inout space (p = 5)\n",
        "\n",
        "if you want to estimate the price of a house with: 2,000 sqft, 5,000 sqft lot, built in 1990, 10 miles city center, and you try local averaging (looking for \"neighbors\" in 5D space.\n",
        "\n",
        "It's very unlikely you'll find many houses exactly matching or even close in all 5 features at once.\n",
        "\n",
        "To get enough neighbors, you'd need to expand your window a lot... but then you're averaging across very different houses (e.g 1,000 sqft vs. 3000 sqft, or 1 bedroom vs, 5 bedrooms).\n",
        "\n",
        "This is the curse of dimensionality:\n",
        "In highesr dimensions, data points are spread far apart.\n",
        "Local neighborhoods have too few points, even if you have a large dataset.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZ1UMcnW0Vtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "What does \"sampled uniformly at random\" mean?\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1iEu4-JQkQDRbMYT1eyA44ZtvWces2K8W)\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1yXETD-EU38ooL6hT2o-p75GAhxZ6k9Vb)\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ImG-ms_z9TP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion of Conditional expecarion and Expirical estimates:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=11ln8NkslEtOULKONuAfyu2ROrKjaE4d3)\n"
      ],
      "metadata": {
        "id": "h0onz_q0Fdgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parametric Models\n",
        "\n",
        "## Curse of Dimensionality\n",
        "The curse of dimensionality (CoD) means that when the number of inputs (features, or p) is large , local averaging around a print x doesnt work well to estimate the regression function f(x).\n",
        "\n",
        "For small p (e.g. 1 or 2 dimensions), you have a decent number of points close to x.\n",
        "\n",
        "As p increases, the number of samples around x quickly drops toward zero.\n",
        "This happens because in higher dimensions, points get farther apart - the data becomes sparse.\n",
        "\n",
        "In 1D, your window around a point is a short line segment ( easy to capture nearby points).\n",
        "\n",
        "In 2D, it's a circle area.\n",
        "\n",
        "In 10D, it's a hypersphere - and most of the data lies at the edges, far away from the center. So local averaging fails because you dont have enough data near your target points.\n",
        "\n",
        "To beat the CoD, we can't rely only on local averages - We need a parametric models.\n",
        "\n",
        "## Parametric Models:\n",
        "\n",
        "The idea:  \n",
        "Instead of trying to average over local neighborhoods, we assume the regression function has a certain **parametric form** (a formula with parameters to estimate).\n",
        "\n",
        "**Linear model example**\n",
        "$$ f_L(x;\\beta) = \\beta_0 + \\beta_1 \\cdot x_1 +  \\beta_2 \\cdot x_2 + ... +  \\beta_p \\cdot x_p $$\n",
        "\n",
        "* Here $ \\beta_0, \\beta_1, \\beta_2, \\beta_3 $ are parameters (weights)\n",
        "* x = ($x_1, x_2, x_3, .... , x_p) is your input vector.\n",
        "\n",
        "\n",
        "**Estimation:**\n",
        "Using your dataset D, you estimate the parameters $\\hat{\\beta}}$\n",
        "\n",
        "Then the estimated function becomes:\n",
        "$$ \\hat{f}_L(x;\\beta) = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\cdot x_1 +  \\hat{\\beta}_2 \\cdot x_2 + ... +  \\hat{\\beta}_p \\cdot x_p $$\n",
        "\n",
        "Why this beats the CoD:\n",
        "* In local averaging, predictions rely only on the few data points near x.\n",
        "* In a parametric model, the prediction uses the whole dataset to estimate the global parameters $ \\hat{\\beta}_i $\n",
        "* that means even if data is sparse in high dimensions, we can still make predictions because the model \"shares strength\" across the entire dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "LsmPujzlH1kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Example of Parametric Models\n",
        "\n",
        "Let's say we have y = 2x + ɛ, with ɛ ~ N(o,1) (some noise).\n",
        "\n",
        "Lets first try 1D (easy for local averaging)\n",
        "\n",
        "Then extend to 5D (curse of dimensionality starts hitting).\n",
        "\n"
      ],
      "metadata": {
        "id": "jI97doO5DVTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 3"
      ],
      "metadata": {
        "id": "Hxu8JF7snDsM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression\n",
        "In this module, we will cover the problem of linear regression.\n",
        "We will start with a formal statement of the problem, we will derive a solution as an **optimization problem**, and provide a closed-form expression using the matrix pseudoinverse. We will then move on to analyze the statistical properties of the linear regression coefficients, such as their covariance and variances.\n",
        "\n",
        "We will use this statistical analysis to determine coefficient accuracy and analyze confidence intervals.\n",
        "We will then move on to the topic of hypothesis testing, which we use to determine dependencies between input variables and outputs.\n",
        "\n",
        "We will then finalize with a collection of metrics to measure model accuracy and a continuation of the introduction to the Python programming language.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "* Understand linear regression problems and their solutions.\n",
        "* Analyze statistical properties of linear coefficients.\n",
        "* Evaluate coefficient accuracy and confidence intervals.\n",
        "* Understand hypothesis testing and implications.\n",
        "* Create the use of Pandas in Python."
      ],
      "metadata": {
        "id": "eCMyTp84nGXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Alegebra:\n",
        "Reference Video:\n",
        "https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\n"
      ],
      "metadata": {
        "id": "0aOcNLZwoATf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Operations:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1vRApRCf7FlYA83-E_xH5Z3l4jAXOz9j0)\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1t8XgzdwlEj2ldwWfmMGbDfZYWCL-A5BU)\n",
        "\n"
      ],
      "metadata": {
        "id": "oLTd3sgpxvTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiplication:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Pv7xULS1GDjQaHvTmdRw9FvTpqNs1yxM)\n"
      ],
      "metadata": {
        "id": "M-KUbmRnu_7y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Matrix multiplication is associative $(AB) C = A (BC)$\n",
        "* Matrix multiplication is distributive $A(B + C )  = AB + AC $\n",
        "* but (in general) not commutative 𝐴𝐵 ≠ 𝐵𝐴"
      ],
      "metadata": {
        "id": "6-nUnMC1vBzc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In linear algebra, when we say matrix multiplication is not commutative, it means:\n",
        "\n",
        "𝐴𝐵 ≠ 𝐵𝐴 in general.\n",
        "\n",
        "🔹 What does this mean?\n",
        "\n",
        "With real numbers, multiplication is commutative: 2×3=3×2.\n",
        "\n",
        "With matrices, the order matters:\n",
        "Multiplying A by B does not usually give the same result as multiplying\n",
        "𝐵 by 𝐴.\n",
        "In fact, sometimes one of the products doesn’t even exist (if dimensions don’t match).\n",
        "![](https://drive.google.com/uc?export=view&id=1Hwm_ERz-rz1LkA-iUaRcsZTo74auz_KK)\n"
      ],
      "metadata": {
        "id": "-F-LTqzHiJUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Transpose & Conjugate:\n",
        "\n",
        "The transpose of a matrix A is denoted by $ A^T $ i.e. $[A^T]_{ij} =[A]_{ji} $.\n",
        "\n",
        "In other words, the transpose results from converting rows of A into columns of $A^T$.\n",
        "The transpose operations satisfies:\n",
        "\n",
        "* $(A^T)^T = A $\n",
        "* $(AB)^T = B^T A^T $\n",
        "* $(A + B) ^ T = A^T + B^T $\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1X03_FZg9HYpHXg0GWdpTRlQrlfKdwuk5)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "C1aNs28QvCqF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question # 1:\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1si4hJcMWbk5t_GMv3q8LeVcCollkSvBl)\n",
        "\n",
        "Both x and y are column vectors:\n",
        "\n",
        "Step # 1: Dimensions\n",
        "* x is 3 x 1\n",
        "* y is 3 x 1\n",
        "* $ x^T $ is 1 x 3\n",
        "\n",
        "Multipluing $ x^T $y =\n",
        "(1 x 3) (3 x 1) = 1 X 1\n",
        "The result is a scalar (a single whole number)\n",
        "\n",
        "$ x^T y $ =\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} 1 & 3 & 5 \\end{bmatrix}\n",
        "\\begin{bmatrix} 2 \\\\ 4 \\\\ 5 \\end{bmatrix}\n",
        "$$\n",
        "$ x^T y $ =  = (1)(2) + (3)(4) + (5)(6) = 2 + 12 + 30 = 44\n",
        "\n"
      ],
      "metadata": {
        "id": "Uo61EmiD60S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 3 - Question # 2"
      ],
      "metadata": {
        "id": "zA3_Wt4E8s1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Week 3 - Question # 3"
      ],
      "metadata": {
        "id": "Fcc6Yt8w8wN5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation:\n",
        "\n",
        "To sum up, linear transformations are a way to move around space such that gridline remain parallel and evenly spaced, and such that the origin remains fixed.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1L28E5lY9sicXTPUVSZE9i2igckPBVt-f)\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1NzujqcU2nvkEuf81qnSDBaD84T_J1g3C)\n",
        "\n",
        "\n",
        "Linear transformation is completely determined by where it takes rhe basis vec tors of the space, which for two dimensions means $ \\hat{i} $ and $ \\hat{j} $.\n",
        "This is because any other vector could de described as a linear combination of those basis vectors.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=19UycmAhRI29BQH8UFrVIrWbDxuwjra6o)\n",
        "\n"
      ],
      "metadata": {
        "id": "hWP5bj2jyOxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Linear Regression - Additive Model\n",
        "\n",
        "\n",
        "Regression analysis is like any other inferential methodology. Our goal is to draw a random sample from a population and use it to estimate the properties of that population.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1ZR8-c1YZz6N07_yzxkz73HMudPSnrY1I)\n",
        "\n",
        "\n",
        "According to this additive model, the output variable y is going to be generated as a function that depends on a vector of inputs x, and a collection of parameters Beta plus some measurement noise.\n",
        "\n",
        "$$ Y = f_L (\\textbf{X};\\beta) + \\varepsilon = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + . . . + \\beta_p X_p + \\varepsilon $$\n",
        "\n",
        "* So the vector bold X contains the variables $ x_1$ , $x_2$ ,  up to $x_p$.\n",
        "$X = (X_1, X_2, . . .X_p)$ are random inputs drawn from some distribution $f_x(x) $.\n",
        "\n",
        "* the coefficients $\\beta_0, \\beta_1,  \\beta_2,  \\beta_3 + . . . + \\beta_p $ are deterministic but unknown coefficients.\n",
        "* the measurement noise follows a distribution $ \\varepsilon ~ f_{\\varepsilon}$\n",
        "\n",
        "✅ In plain words:\n",
        "“The additive model induces a joint PDF” means: once you assume\n",
        "𝑌 is generated by a linear function of 𝑋 plus random noise, you automatically define how the pair (𝑋,𝑌) is distributed together — i.e., their joint probability density function."
      ],
      "metadata": {
        "id": "gI3vbzt2PGbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Notice that given the marginal of x and the equation of the additive model, we can compute a joint PDF, f(x, y).\n",
        "\n",
        "HERE IS HOW TO DO THAT:\n",
        "\n",
        "**Step # 1: Restate the additive model**\n",
        "\n",
        "$$ Y = f_L = (\\textbf{X};\\beta) + \\varepsilon = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + . . . + \\beta_p X_p $$\n",
        "\n",
        "Where\n",
        "* X ~ $f_X(x)$ (we knpw the marginal distributional of the predictors)\n",
        "* $ \\varepsilon ~ f_{\\varepsilon}(\\varepsilon) $ (independent of X).\n",
        "\n",
        "**Step # 2: Express conditional distribution:**\n",
        "Given X = x,\n",
        "$$ Y |X = x = \\beta_0 + \\beta^T x + \\varepsilon $$\n"
      ],
      "metadata": {
        "id": "-e6uLIDjdGD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Quick Note on ~\n",
        "Meaning of ~\n",
        "$$ (x_i, y_i) ~ f_{XY} $$\n",
        "\n",
        "it means:\n",
        "* The random vector $(x_i, y_i)$ is distributed according to the probablility distribution with density $f_{XY}$\"\n",
        "In other words: $ (x_i, y_i) $ follows the joint distribution $f_{XY}$\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1cVNNZW2GHViMoZ6XrKYfEcuA7pjlw3JG)\n"
      ],
      "metadata": {
        "id": "BfXQYe7Ve_g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Regression Problem\n",
        "\n",
        "* Given a training dataset $D_{Tr}$ - ${(x_i, y_i)}^N_{i=1}$ consisting of N independent samples $ (x_i, y_i) $ ~ $f_{XY} $\n",
        "* Estimate values for unknown coefficients $ \\beta_0, \\beta_1, \\beta_2, . ..., \\beta_p $ denoted y $\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, ...., \\hat{\\beta}_p$. Because $\\beta$ values are known by nature, we will use $ \\hat{\\beta}} $\n",
        "* Once we have these estimates, we can make prediction about the output variable corresponding to a new input $ x = [x_1, ...., x_p] ^T$ , as follows:\n",
        "$$ \\hat{y} = f_L (\\textbf{x};\\hat{\\beta}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + . . . + \\hat{\\beta}_p x_p $$"
      ],
      "metadata": {
        "id": "pzWcMGRMefo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Sum of the Square Error:\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1XW2n5nfjQJ0UDYkmqjnYqvjK-0jm1RLN)\n",
        "\n",
        "When you square the difference between blue dots and yellow line. and then same it all - it produces Sum of the Squared Error and we need to find the $\\beta_0 $ and $ \\beta_1 $ that minimizes number."
      ],
      "metadata": {
        "id": "8C-10z2AdxJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Calculate Coefficients $\\beta_0$ & $\\beta_1$ in One predictor Variable\n",
        "\n",
        "  We want to find a way where we can use the known data points to estimate the values of $ \\beta_0 $ & $ \\beta_1 $ and the estimates that we obtain will be represented by this  $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $.\n",
        "  we would use some function of the data that we have collected to estimate $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $. of-course your estimate could be so good that your estimates turn out to be exactly $ {\\beta} $ and $ \\hat{\\beta} $ and in that case  your white line with co-incide with the red line entiry. but in reality we get a slight deviation.\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1oO1GK2GDPDN-jvJ6GZR8MRDjcut2GEhc)\n",
        "\n",
        "  So the question now is how do we obtain the best estimate of $\\hat{\\beta}_0$ & $\\hat{\\beta}_1$\n",
        "\n",
        "\n",
        "$$ \\hat{\\beta}_1 = \\frac{\\sum^n_{i=1} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum^n_{i=1} (x_i - \\bar{x})^1} $$\n",
        "\n",
        "To calculate $\\hat{\\beta_0}}$\n",
        "\n",
        "$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} $$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The residual is:\n",
        "$$ e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i ) $$\n",
        "\n",
        "Where $Y_i$ is the actual Y value and $\\hat{Y}_i $ is the estimated one.\n",
        "\n",
        "$$ e_i = Y_i - \\hat{Y}_i $$\n",
        "\n",
        "Using the above statement we can find the sum of all the residual - but doing so could actually give a false positive. In order to get around this problem - we do the square of sum of residual. so that residual can nto offste each other.\n",
        "\n",
        "\n",
        "some of the residual could be possitive and might have negative values to cancel them off or offset each other.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1UMHRyFAn2H64Y86jLHbpG-CAkIAUFUv6)\n",
        "\n",
        "Even though the fit in above image is not good, but summing all residual and picking the one with least square sum will give you a very good fit.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iwEHuXEArAHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Coefficients $\\beta_0$ & $\\beta_1$ in multiple predictor Variable\n",
        "## Linear Regression using Matrix\n",
        "\n",
        "The Equation:\n",
        "\n",
        "$$ \\hat{\\beta} = (M_X^T M_X)^{-1} M_X^T y $$\n",
        "\n",
        "Where\n",
        "* $ \\hat{\\beta} $ : This is the vector of estimated coefficients. It has values $\\beta_0$, $\\beta_1$ , $\\beta_2$.\n",
        "\n",
        "* $ \\hat{\\beta}_0 $ = intercept (\"The starting value\" on Y-axis.\n",
        "* $ \\hat{\\beta}_1 $ , $ \\hat{\\beta}_2 $ = slopes for each predictor variables.\n",
        "* $M_X $ This is the design matrix. It’s basically a big table that holds all your input x-values:\n",
        "\n",
        "** The first column is all 1s (so the model can include the intercept).\n",
        "\n",
        "** The other columns are your predictor variables ( $x_1$, $x_2$, ...).\n",
        "\n",
        "* y: This is your outcome (dependent variable) vector.\n",
        "* $(M_X^T M_X)^{-1} M_X^T y $ This is called the Moore–Penrose pseudoinverse of the matrix M_X. It’s the mathematical trick that gives you the “best fit” line through your data points in the least squares sense.\n",
        "\n"
      ],
      "metadata": {
        "id": "MaMbqhhXqzaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Note on Optimization\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1QE5VDvl69IXPTvqt0llgKZbfFheN3zZc)\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1QE5VDvl69IXPTvqt0llgKZbfFheN3zZc)"
      ],
      "metadata": {
        "id": "mOIDPWyWkHfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Univariate Linear Regression\n",
        "Univariate is one indepedent variables (one predictor)\n",
        "The model lookslike:\n",
        "\n",
        "$$ y = \\beta_0 + \\beta_1 x + \\varepsilon $$\n",
        "where\n",
        "* y = dependent (output) variable.\n",
        "* x = independent (input) variable (the predictor).\n",
        "* $\\beta_0 , \\beta_1$ = coefficients we want to estimate\n",
        "* $\\varepsilon $ = random error\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1jIP9wDuFZKxTFlErxFWTcIHkLk3BJV9h)"
      ],
      "metadata": {
        "id": "mV5WGxevoivK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "reference video: https://www.youtube.com/watch?v=54ewnkdWU6w&t=496s\n",
        "\n",
        "# Confidence Interval\n",
        "\n",
        "We can compute $\\hat{\\beta}_0 $ , . . . $\\hat{\\beta_0}$ from D alone. These are estimate aiming to approximate the values of $\\beta_0$ and $\\beta_p$ using data.\n",
        "\n",
        "These estimates are random variables that are centered around each one of the parameters, Beta\n",
        "\n",
        "\n",
        "Analyze the uncertainty of our estimates using confidence Intervals, in other words, we can claim that the true value of a parameter $\\beta_i$ is within a particular interval with a 95% probability.\n",
        "\n",
        "The parameter $ \\hat{\\beta}_i$ is distributed according to a Gaussian distribution that is centered around the true value of $\\beta_i$.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Slope $\\beta_1$  & intercept $ \\beta_0 $: for every unit increase in slope, we would see increase the intercept.\n",
        "\n",
        "\n",
        "reference video: https://www.youtube.com/watch?v=54ewnkdWU6w&t=496s\n",
        "\n",
        "Example in the given video teaches us that, when we work with samples, the point estimare of the coefficient is not necessarily (and almost never is) the true value.\n",
        "\n",
        "Instead of using the point estimare, it's always better to work with ranges of values for out estimates, which we call confidence intervals.\n",
        "\n",
        "It is standard to work with 05% confidence intervals, which means we are 95% certain the true value lies within our interval.\n",
        "\n",
        "\n",
        "$$ \\beta_i = [\\hat{\\beta}_i - 2 \\cdot SD (\\hat{\\beta}_i), \\beta_i + 2 \\cdot SD (\\hat{\\beta}_i) ] $$\n",
        "\n",
        "What we can claim is that the value that nature uses, essentially this $ \\beta_i $  that is unknown to us is contained within this interval with a 95 percent probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7sgA_I9jLgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Hypothesis Testing:\n",
        "\n",
        "Deterines how likely it is for a particular input $ X_i $ to influence the output Y. In this task, we used Hypothesis Testing, which allow us to build statistical evidence to reject Null Hypothesis Testing, which allow us to build statistical evidence to reject Null Hypothesis of the form: $X_i$ does not influence Y (Legal parable Bob did not kill Alice)\n",
        "\n"
      ],
      "metadata": {
        "id": "nI_FlwSlw8OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Categorical Inputs\n",
        "\n",
        "Recap:\n",
        "\n",
        "1. The Linear Regression Model\n",
        "\n",
        "The model is usually written as (for 1 dimensional model) :\n",
        "\n",
        "$$ Y = \\beta_0 + \\beta_1 X + \\varepsilon $$  \n",
        "\n",
        "(for multi dimensional model) :\n",
        "\n",
        "\n",
        "$$ Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\varepsilon $$  \n",
        "\t​\n",
        "\n",
        "Where:\n",
        "* $\\beta_0$ (intercept): where the regression line crosses the y-axis (the value of y when x = 0).\n",
        "\n",
        "* $\\beta_1$ (Slope): how much y changes, on average, for a one-unit increase in x.\n",
        "\n",
        "$$ \\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\varepsilon $$\n",
        "\n",
        "with “hats” showing estimated values.\n",
        "\n",
        "Where $x_i$ is a p-dimensional vector and $y_i$ is a scalar output.\n",
        "\n",
        "These random input vectors are drawn from a distribution f_x, which is the marginal distribution of the input vector x_i.\n",
        "\n",
        "\n",
        "**The Key Idea**\n",
        "\n",
        "In simple regression, X is just $X_1$. Easy to plot.\n",
        "In multiple regression, there isn't one single \"X- axis\" - the predictors together define a higher-dimensional space.\n",
        "\n",
        "\n",
        "That’s why in textbooks you usually see either:\n",
        "\n",
        "Plots of the fitted line/plane with 1 or 2 predictors, or\n",
        "\n",
        "Plots that show the effect of one predictor while holding others fixed.\n",
        "\n",
        "---\n",
        "1. Input Vectors in Machine Learning / Statistics\n",
        "\n",
        "Suppose you’re working with data:\n",
        "\n",
        "$$ (x_i, y_i), = i = 1, . . . , n $$\n",
        "\n",
        "$ x_i $ = input vector (features), e.g. age, income, blood pressure.\n",
        "\n",
        "$ y_i $ = output (label/response), e.g. disease outcome.\n",
        "\n",
        "so, $ x_i $ itself is a random vector because it comes from some population, not fixed by you.\n",
        "\n",
        "\n",
        "2. Marginal Distribution of x\n",
        "In probability, when we talk about the joint distribution of (x,y), we mean a probability law that governs both input and output together.\n",
        "\n",
        "If you only care about the inputs x, you look at the marginal distribution of\n",
        "x:\n",
        "\n",
        "$$ f_x(x) = \\int f_{x,y}(x,y)dy $$\n",
        "This \"marginalizes out the y-part, leaving just the distribution for the inputs.\n"
      ],
      "metadata": {
        "id": "GjFeHJPOui5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "rdE3vZc3sv5A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "# Qualitative Inputs:\n",
        "\n",
        "Linear models can handle qualitative inputs, also called categorical variables, taking a discrete set of values.\n",
        "\n",
        "\n",
        "Analyze the differences in credit card balance between males and females, ignoring other variables. The output variables $ y_i $ represents the credit card balance of individual i. We will consider the gender of individual i as the only input $x_i$. How do we build a linear model?\n",
        "\n",
        "Step # 1: Create a dummy variables:\n",
        "x_i = 1 if individual i is a female and 0 is individual i is a male\n",
        "\n",
        "Step # 2:\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1naTsJnlThcTuG0e7kOLhLPpXrnCcTxNY)\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1tj13cskfjBLGAoXeR06WDdHzQs4tayxu)\n",
        "\n",
        "\n",
        "\n",
        "Reference video: https://www.youtube.com/watch?v=9yTui_LoSOc\n"
      ],
      "metadata": {
        "id": "K0ovDxlyx8ac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Problems"
      ],
      "metadata": {
        "id": "qxXckH9g8HBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate $M_X^T$ from a Dataset to calculate $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "Given a dataset $D_{Tr}$ = ${(x_i, y_i)}_{i=1}^4 = {(1,2),(2,5),(3,13),(4,20)}$\n",
        "\n",
        "Co-efficients of a quadractic model =\n",
        "$$ \\hat{Y} = \\hat{\\beta_0} + \\hat{\\beta}_1 X + \\hat{\\beta}_1 X^2 $$\n",
        "\n",
        "A Simple linear regression ( intercept + x )\n",
        "\n",
        "x = [1,2,3,4]\n",
        "\n",
        "Fopr a Quadractic model (intercept + x + $x^2$)\n",
        "\n",
        "\n",
        "\n",
        "$ M_x$ =\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1\\\\\n",
        "1 & 2 & 4\\\\\n",
        "1 & 3 & 9\\\\\n",
        "1 & 4 & 16\\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "Transposing the Matrix\n",
        "\n",
        "$ M_X^T  $ =\n",
        "\n",
        "\\begin{bmatrix}\n",
        "1 & 1 & 1 & 1\\\\\n",
        "1 & 2 & 3 & 4\\\\\n",
        "1 & 3 & 9 & 16\\\\\n",
        "\\end{bmatrix}\n",
        "\n",
        "\n",
        "# Calculate $M_X^T$ $M_X$ from a Dataset to calculate $\\beta_0$ and $\\beta_1$\n",
        "\n",
        "Multiplication:\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1an4Tpl2tdo3XWv2zGwMlopoax5FRfvBB)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EZgRajCg8Jne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mean Squared Error:\n",
        "It measures how far off your model's predictions are from the true values.\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$ MSE = \\frac{1}{n} \\sum^n_{i=1} (y_i - \\hat{y}_i)^2 $$\n",
        "\n",
        "Where\n",
        "  * $y_i$ = actual (true) value\n",
        "  * $\\hat{y}_i$ = predicted value\n",
        "  * n = number of data points\n",
        "\n",
        "🔹 Why square the error?\n",
        "\n",
        "* If you just subtract ( $ (y_i - \\hat{y}_i) $ ), positive and negative errors could cancel out.\n",
        "\n",
        "* Squaring makes all errors positive and penalizes large mistakes more.\n",
        "\n",
        "* Then we average, so it’s on a comparable scale across models.\n",
        "\n",
        "\n",
        "🔹 Intuition\n",
        "\n",
        "Imagine you're predicting house prices:\n",
        "  * True prices: $200k, Prediction: $210K -> Error = 200 - 210 = -10K , squared = 100M\n",
        "  * True price: $200k, Prediciton $250K -> -50K, squared = 2.5B\n",
        "\n",
        "  The big mistake is punished way more: That's why MSE is sensitive to outliers.\n",
        "\n",
        "  🔹 In practice\n",
        "\n",
        "Low MSE = predictions close to reality (good model).\n",
        "\n",
        "High MSE = predictions are often far off (bad model).\n"
      ],
      "metadata": {
        "id": "jlVfHyyvxcGH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lyStLVcNvqOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASSIFICATION\n",
        "\n",
        "Consider a discret set C, which contains K class labels. In Classification problems, the output variable Y is qualitative and takes values from C.\n",
        "\n",
        "The Goal is usually to build a classifier C(x) that assigns a class label into C to an input x\n",
        "\n",
        "for example:\n",
        "C = [\"Cats\", \"Dogs\", \"Parrots\"... \"k\" ]\n",
        "$$ C: \\mathbb{R}^P \\longmapsto C $$\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=18UG60E6ZOE4ni4O4Q4BJo3UdlUPq4D2a)\n",
        "\n",
        "## 🔹 Generative Model Idea\n",
        "  The generative model assumes that the data $(x_i, y_i)$ are generated in two steps:\n",
        "  1- Sample the input $ x_i $ First, draw  $ x_i $  from the marginal probability distribution of inputs:\n",
        "  $$ x_i ~ f_X(x) $$\n",
        "\n",
        "  Where $ f_X $ is the marginal probability density function (PDF).\n",
        "  This step says: \"inputs come from some underlying distribution of data (like natural images, speech signals, etc.).\n",
        "\n",
        "  2- Sample the output $ y_i $\n",
        "  Next, given the sampled input $ x_i $, we draw the label $ y_i $, according to a conditional probability mass function (PMF):\n",
        "\n",
        "  $$  p_k (x_i) = Pr(Y = k | X = x_i ),  $$\n",
        "  for all k ∈ C\n",
        "\n",
        "  This is called the conditional class probability. It tells us how likley each class label is, given the input.\n",
        "\n",
        "  ✅ So in simple words:\n",
        "\n",
        "Drawing from $ f_X $ = “pick a random image from the universe of possible images.”\n",
        "\n",
        "Then, given that image, assign a label (“cat” or “dog”) with probability\n",
        "$ Pr(Y = k | X = x_i )  $.\n"
      ],
      "metadata": {
        "id": "pLi9Mujdxbc5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1czGO-9JUt6BqvceuPHK1XRTa87o4LjmR)\n",
        "\n",
        "  The y-axis represents the conditional class probability\t​(x)=Pr(Y=1∣X=x),\n",
        "\n",
        "  i.e. the probability that the label is 1 given the input x.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1GWkQ4i2cT51-EWsrbo_z5LS3xVQLyixv)\n",
        "  "
      ],
      "metadata": {
        "id": "Kk1Ouk1WATPp"
      }
    }
  ]
}