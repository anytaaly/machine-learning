{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1dhKpYPcrvCkns9qz0R6dCcUNeOkr9KHc",
      "authorship_tag": "ABX9TyNnpKPsIOeogmAE/7ppI89M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anytaaly/machine-learning/blob/main/Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eigenvector:  \n",
        "\n",
        "If a vector is multiplied with a matrix and it doesn't change it's direction. Thus, vector v is therefore an eigenvector.\n",
        "\n",
        "# Eigenvalue:\n",
        "The eigenvalue tells us how much the eigenvector changes in size when multiplied with the matrix.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1Vfhc1zlVmCzevzrbQQOijyGexNtViq65)\n",
        "\n",
        "Note that, for a symmetric matrix like this one, we could have multiple eigenvectors. Two eigenvectors of a symmetric matrix will be orthogonal.\n",
        "\n",
        "which means that the angle between these two vectors will be 90 degrees.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1f43RAgBQFlnB2QWio_us2-7vXOlDHChM)\n",
        "\n"
      ],
      "metadata": {
        "id": "_0wkKhbAM5b5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformation:\n",
        "\n",
        "To sum up, linear transformations are a way to move around space such that gridline remain parallel and evenly spaced, and such that the origin remains fixed.\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1L28E5lY9sicXTPUVSZE9i2igckPBVt-f)\n",
        "\n",
        "\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1NzujqcU2nvkEuf81qnSDBaD84T_J1g3C)\n",
        "\n",
        "\n",
        "Linear transformation is completely determined by where it takes rhe basis vec tors of the space, which for two dimensions means $ \\hat{i} $ and $ \\hat{j} $.\n",
        "This is because any other vector could de described as a linear combination of those basis vectors.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=19UycmAhRI29BQH8UFrVIrWbDxuwjra6o)\n",
        "\n"
      ],
      "metadata": {
        "id": "hWP5bj2jyOxp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# ----------------------------------------------> Linear Regression - Additive Model\n",
        "\n",
        "Quick Intro of linear Regression: https://www.youtube.com/watch?v=3dhcmeOTZ_Q\n",
        "\n",
        "GPA Example - Linear Regression: https://www.youtube.com/watch?v=Muw2GjZ0Xww\n",
        "\n",
        "\n",
        "\n",
        "Regression analysis is like any other inferential methodology. Our goal is to draw a random sample from a population and use it to estimate the properties of that population.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1ZR8-c1YZz6N07_yzxkz73HMudPSnrY1I)\n",
        "\n",
        "\n",
        "According to this additive model, the output variable y is going to be generated as a function that depends on a vector of inputs x, and a collection of parameters Beta plus some measurement noise.\n",
        "\n",
        "$$ Y = f_L (\\textbf{X};\\beta) + \\varepsilon = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + . . . + \\beta_p X_p + \\varepsilon $$\n",
        "\n",
        "* So the vector bold X contains the variables $ x_1$ , $x_2$ ,  up to $x_p$.\n",
        "$X = (X_1, X_2, . . .X_p)$ are random inputs drawn from some distribution $f_x(x) $.\n",
        "\n",
        "* the coefficients $\\beta_0, \\beta_1,  \\beta_2,  \\beta_3 + . . . + \\beta_p $ are deterministic but unknown coefficients.\n",
        "* the measurement noise follows a distribution $ \\varepsilon ~ f_{\\varepsilon}$\n",
        "\n",
        "âœ… In plain words:\n",
        "â€œThe additive model induces a joint PDFâ€ means: once you assume\n",
        "ð‘Œ is generated by a linear function of ð‘‹ plus random noise, you automatically define how the pair (ð‘‹,ð‘Œ) is distributed together â€” i.e., their joint probability density function.\n",
        "\n",
        "$$ f_{Y|X} (y|x) = \\frac{f_{X,Y}(x,y)}{f_X(x)} for F_X(x) > 0 $$\n",
        "\n",
        "We mean\n",
        "* we start with an assumption\n",
        " Y = f(x) + $ \\varepsilon $, where  $ \\varepsilon $ is random noise.\n",
        " * This assumption implies a probabilistic relationship between X and Y.\n",
        " * X has its own distribution (maybe uniform, Guassina, Categorical etc.)\n",
        " * The noise $ \\varepsilon $ has its own distribution (often Guassian).\n",
        " * Together, these define how (X,Y) are distributed jointly.\n",
        "\n",
        " So the join PDF is just the mathematical way of saying: \"Here's the probability of seeing a particular combination of X and Y.\"\n",
        "  It mean that X and Y are related with some probability function, and if you know X and a probability, you can determine Y.\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=14KJeKhjJ5WiV0pEUbpzVzTTrS0IvoKyZ)\n",
        "\n"
      ],
      "metadata": {
        "id": "gI3vbzt2PGbh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Notice that given the marginal of x and the equation of the additive model, we can compute a joint PDF, f(x, y).\n",
        "\n",
        "HERE IS HOW TO DO THAT:\n",
        "\n",
        "**Step # 1: Restate the additive model**\n",
        "\n",
        "$$ Y = f_L = (\\textbf{X};\\beta) + \\varepsilon = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\beta_3 X_3 + . . . + \\beta_p X_p $$\n",
        "\n",
        "Where\n",
        "* X ~ $f_X(x)$ (we knpw the marginal distributional of the predictors)\n",
        "* $ \\varepsilon ~ f_{\\varepsilon}(\\varepsilon) $ (independent of X).\n",
        "\n",
        "**Step # 2: Express conditional distribution:**\n",
        "Given X = x,\n",
        "$$ Y |X = x = \\beta_0 + \\beta^T x + \\varepsilon $$\n"
      ],
      "metadata": {
        "id": "-e6uLIDjdGD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Quick Note on ~\n",
        "Meaning of ~\n",
        "$$ (x_i, y_i) ~ f_{XY} $$\n",
        "\n",
        "it means:\n",
        "* The random vector $(x_i, y_i)$ is distributed according to the probablility distribution with density $f_{XY}$\"\n",
        "In other words: $ (x_i, y_i) $ follows the joint distribution $f_{XY}$\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1cVNNZW2GHViMoZ6XrKYfEcuA7pjlw3JG)\n"
      ],
      "metadata": {
        "id": "BfXQYe7Ve_g6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------------------------------> Linear Regression Problem\n",
        "\n",
        "* Given a training dataset $D_{Tr}$ - ${(x_i, y_i)}^N_{i=1}$ consisting of N independent samples $ (x_i, y_i) $ ~ $f_{XY} $\n",
        "* Estimate values for unknown coefficients $ \\beta_0, \\beta_1, \\beta_2, . ..., \\beta_p $ denoted by $\\hat{\\beta}_0, \\hat{\\beta}_1, \\hat{\\beta}_2, ...., \\hat{\\beta}_p$. Because $\\beta$ values are known by nature, we will use $ \\hat{\\beta} $\n",
        "* Once we have these estimates, we can make prediction about the output variable corresponding to a new input $ x = [x_1, ...., x_p] ^T$ , as follows:\n",
        "\n",
        "$$ \\hat{y} = f_{l} (\\textbf{x};\\hat{\\beta}) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_1 + \\hat{\\beta}_2 x_2 + \\hat{\\beta}_3 x_3 + . . . + \\hat{\\beta}_p x_p $$\n",
        "\n",
        "\n",
        "\n",
        "To find the vector of estimated co-efficients\n",
        "\n",
        "$$ \\hat{\\beta} = [\\hat{\\beta_0},\\hat{\\beta_1}, ...., \\hat{\\beta_p}] $$\n",
        "\n",
        "from $D_{Tr} = {(x_i, y_i)}^N_{i=1} $\n",
        "\n",
        "\n",
        "We solve the following optimization problem:\n",
        "\n",
        "## **Ordinary Least Square - Argmin form:**\n",
        "\n",
        "$$ \\hat{\\beta} = arg min_{\\beta_0 \\beta_1} \\sum_{i=1}^{N} (y_i - fL(x_i; \\beta ))^2 $$\n",
        "\n",
        "## **Ordinary Least Square - Loss (RSS) form:**\n",
        "\n",
        "$$ L = \\sum^{n}_{i=1} (\\varepsilon_i)^2 = \\sum^{n}_{i=1} (y_i - \\beta_0 - \\beta_1 x_i )^2 $$\n",
        "\n",
        "\n",
        "ABOVE EQUATION GIVES US BEST LINE FIT: WE DO THIS BY THE METHOD OF LEAST SQUARES:\n",
        "\n",
        "The summation above is called the Residual Sum of Squares (RSS).\n",
        "\n",
        "When you square the difference between blue dots and yellow line. and then same it all - it produces Sum of the Squared Error:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pzWcMGRMefo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Coefficients $\\beta_0$ & $\\beta_1$ in multiple predictor Variable\n",
        "## Linear Regression using Matrix\n",
        "\n",
        "The Equation:\n",
        "\n",
        "$$ \\hat{\\beta} = (M_X^T M_X)^{-1} M_X^T y $$\n",
        "\n",
        "Where\n",
        "* $ \\hat{\\beta} $ : This is the vector of estimated coefficients. It has values $\\beta_0$, $\\beta_1$ , $\\beta_2$.\n",
        "\n",
        "* $ \\hat{\\beta}_0 $ = intercept (\"The starting value\" on Y-axis.\n",
        "* $ \\hat{\\beta}_1 $ , $ \\hat{\\beta}_2 $ = slopes for each predictor variables.\n",
        "* $M_X $ This is the design matrix. Itâ€™s basically a big table that holds all your input x-values:\n",
        "\n",
        "** The first column is all 1s (so the model can include the intercept).\n",
        "\n",
        "** The other columns are your predictor variables ( $x_1$, $x_2$, ...).\n",
        "\n",
        "* y: This is your outcome (dependent variable) vector.\n",
        "* $(M_X^T M_X)^{-1} M_X^T y $ This is called the Mooreâ€“Penrose pseudoinverse of the matrix M_X. Itâ€™s the mathematical trick that gives you the â€œbest fitâ€ line through your data points in the least squares sense.\n",
        "\n",
        "The inverse of 2 x 2 matrix is computed as follows:\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1kx-S3qOBtzUmJu3ldsFz4EwHvOaXgJa4)"
      ],
      "metadata": {
        "id": "MaMbqhhXqzaU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------------------------------> Univariate Linear Regression\n",
        "Univariate is one indepedent variables (one predictor)\n",
        "The model lookslike:\n",
        "\n",
        "$$ y = \\beta_0 + \\beta_1 x + \\varepsilon $$\n",
        "where\n",
        "* y = dependent (output) variable.\n",
        "* x = independent (input) variable (the predictor).\n",
        "* $\\beta_0 , \\beta_1$ = coefficients we want to estimate\n",
        "* $\\varepsilon $ = random error\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1jIP9wDuFZKxTFlErxFWTcIHkLk3BJV9h)\n",
        "\n",
        "\n",
        "    ![](https://drive.google.com/uc?export=view&id=1YAH3gqZ8pizI97V2FKnnwBOBek5UR_b4)\n"
      ],
      "metadata": {
        "id": "mV5WGxevoivK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Calculate Coefficients $\\beta_0$ & $\\beta_1$ in One predictor Variable\n",
        "\n",
        "  \n",
        "  Reference vide: GPA ExmPLE: https://www.youtube.com/watch?v=M-f2c3p-kA0\n",
        "  \n",
        "  \n",
        "  We want to find a way where we can use the known data points to estimate the values of $ \\beta_0 $ & $ \\beta_1 $ and the estimates that we obtain will be represented by this  $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $.\n",
        "  \n",
        "  \n",
        "  we would use some function of the data that we have collected to estimate $ \\hat{\\beta}_0 $ and $ \\hat{\\beta}_1 $. of-course your estimate could be so good that your estimates turn out to be exactly $ {\\beta} $ and $ \\hat{\\beta} $ and in that case  your white line with co-incide with the red line entiry. but in reality we get a slight deviation.\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1oO1GK2GDPDN-jvJ6GZR8MRDjcut2GEhc)\n",
        "\n",
        "  So the question now is how do we obtain the best estimate of $\\hat{\\beta}_0$ & $\\hat{\\beta}_1$\n",
        "\n",
        "\n",
        "  $$ \\hat{\\beta}_1 = \\frac{\\sum^n_{i=1} (x_i - \\bar{x}) (y_i - \\bar{y})}{\\sum^n_{i=1} (x_i - \\bar{x})^2} $$\n",
        "\n",
        "  To calculate $\\hat{\\beta_0}$\n",
        "\n",
        "  $$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1\\bar{x} $$\n",
        "\n"
      ],
      "metadata": {
        "id": "iwEHuXEArAHi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Note on Optimization\n",
        "\n",
        " ![](https://drive.google.com/uc?export=view&id=1QE5VDvl69IXPTvqt0llgKZbfFheN3zZc)\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1QE5VDvl69IXPTvqt0llgKZbfFheN3zZc)"
      ],
      "metadata": {
        "id": "mOIDPWyWkHfx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ----------------------------------------------> Sum of the Square Error:\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1XW2n5nfjQJ0UDYkmqjnYqvjK-0jm1RLN)\n"
      ],
      "metadata": {
        "id": "moKBzGwn_3WM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## Model Adequacy - How good is the model?\n",
        "\n",
        "Reference for below content: https://www.youtube.com/watch?v=yi9-D9l1_ag\n",
        "\n",
        "The residual is:\n",
        "$$ e_i = Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i ) $$\n",
        "\n",
        "Where $Y_i$ is the actual Y value and $\\hat{Y}_i $ is the estimated one.\n",
        "\n",
        "$$ e_i = Y_i - \\hat{Y}_i $$\n",
        "\n",
        "Using the above statement we can find the sum of all the residual - but doing so could actually give a false positive. In order to get around this problem - we do the square of sum of residual. so that residual can nto offste each other.\n",
        "\n",
        "\n",
        "some of the residual could be possitive and might have negative values to cancel them off or offset each other.\n",
        "\n",
        "Even though the fit in above image is not good, but summing all residual and picking the one with least square sum will give you a very good fit.\n",
        "\n",
        "\n",
        "We will u se the residuals to provide information about the adequacy of the model:\n",
        "\n",
        "Residual sum of squares\n",
        "\n",
        "$$  RSS = SS_E = (\\sum^{n}_{i=1}(y_i - \\hat{y_i})^2  = \\sum^{n}_{i=1} e^{2}_i $$\n",
        "\n",
        "\n",
        "$$  RSE = \\sqrt{\\frac{RSS}{N-p-1}} $$\n",
        "\n",
        "$$  TSS = \\sum^n_{i=1} (y_i - \\bar{y} )^2 $$\n",
        "\n",
        "\n",
        "\n",
        "We can use our residual sum of sqaured, SSE, to get the estimates for the variance using:\n",
        "\n",
        "an estimate for $ \\sigma^2 $: We have 2 parameters and thats why 2 in the equation:\n",
        "\n",
        "$$ \\hat{} {\\sigma} ^2 = \\frac{SS_E}{n-2} $$\n",
        "\n",
        "\n",
        "We can use standard Error for our intercept and slope:\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1-GToKH5F7qolm_YJuZhLV0W9m-gJ0KdW)\n",
        "\n",
        "\n",
        "\n",
        "Mean Analysis is: for all i (unbiased estimator):\n",
        "\n",
        "$$ [\\hat{\\beta_i}] = {\\beta_i} $$\n",
        "\n",
        "Covariance Analysis:\n",
        "\n",
        "Assuming that we are given a data matrix $M_x$, the covariance matrix of $ \\hat{\\beta} $ satiesfied without proof:\n",
        "\n",
        "Cov$[ \\hat{ \\beta } | M_x ] = \\sigma^{2}  ( M_T M_x )^{-1} $\n"
      ],
      "metadata": {
        "id": "ZI3Kr2y8OPcM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# ------------------------> Confidence Interval\n",
        "\n",
        "An Introduction to Statistical Learning:\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1CaZeYvo0MaRnfatlrTdoUFGquSeHjxu6)\n",
        "\n",
        "\n",
        "In an experiement if we complete repeated samples/experiments drawn from the same data-generating process, the OLS estimate clusters around the true coefficient. The expectation is indeed the mean of that sampling distribution.\n",
        "\n",
        "A similar statement holds for  $\\beta_0 $ (not shown in the figure): under the usual OLS assumptions, $ \\mathbb{E}[\\hat{\\beta_1}] =0.5 $, with its own standard error.\n",
        "\n",
        "The PDF of $\\hat{\\beta_1}$ is a normal distribution.\n",
        "\n",
        "Even though we will never be able to exactly retrieve the value of beta1 that Nature is using to generate the data that we observe, we can build confidence intervals based on data following the steps below.\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1E6W3P3yi2j_seID7Hkmu6vRJTffiMLBu)\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1rHiH65mT9pBhFT-yoPK5epuCJ1TN7-Ia)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y7sgA_I9jLgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question: Calculate 95% confidence interval**\n",
        "\n",
        "For a 95% two-sided confidence interval, the total tail probability is a = 1 - 0.95 = 0.05\n",
        "\n",
        "We Splitit equally two tails, so each tail gets a/2 = 0.025\n",
        "\n",
        "The Critical value is the t (or z) quantile at:\n",
        "\n",
        "$$  1 - a/2 = 1 - 0.025 = 0.975 $$\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1ClLsbzxfU0Kak1p2nl9_nyTN6Mz_BF_g)\n",
        "\n",
        "\n",
        "In your Advertising example, nâ‰ˆ200 and  p=3 predictors â†’\n",
        "\n",
        "df=nâˆ’(p+1)=196, and $ t_{0.975,196} $ â‰ˆ1.97.\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=1aSYtoJUX3wNZXLGHeF-6ICmAVhx3QnVN)\n",
        "\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=13yf7InLnGbODJFoV1YpLat3YSukpen4m)\n",
        "\n",
        "\n",
        "\n",
        "* Intercept: co-efficient(2.939)  +- 1.97 = (2.325, 3.553)\n",
        "* TV: co-efficient(0.046)  +- 1.97 = (0.0432,0.0488)\n",
        "* Radio: co-efficient(0.189)  +- 1.97 = (0.1721, 0.2059)\n",
        "* news paper: co-efficient(-0.001)  +- 1.97 = (-0.0126, 0.0106)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZJxHnGqJ9U7C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ----------------------> HYPOTHESIS TESTING\n",
        "\n",
        "\n",
        "  ![](https://drive.google.com/uc?export=view&id=11Lsz-kal7RuCylgE_y-Hl6_NJcC_jjm-)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uTrqBjI5dLZr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Hypothesis Testing:\n",
        "\n",
        "Determines how likely it is for a particular input $ X_i $ to influence the output Y.\n",
        "\n",
        "In this task, we used Hypothesis Testing, which allow us to build statistical evidence to reject Null Hypothesis Testing of the form: $X_i$ does not influence Y (Legal parable Bob did not kill Alice).\n",
        "\n",
        "* Whenever $X_1$ does not influence Y, we have that $ \\beta_1 $ = 0 and Y = $\\beta_0 $ + $ \\varepsilon $\n",
        "\n",
        "* One might be temped to answer this question by computing the estimate $ \\hat{\\beta_1} $ and check if its value is zero.\n",
        "\n",
        "We state two hypotheses:\n",
        "\n",
        "* A null hypothesis, denoted by $H_0$, stating that there is no relationship between $ X_1 $ and Y (i.e. $\\beta $ = 0 )\n",
        "\n",
        "* An alternative hypothesis, denoted by $ H_a$ , stating there is a relationship between $X_1$ and Y"
      ],
      "metadata": {
        "id": "nI_FlwSlw8OW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Null Hypothesis\n",
        "In statistics, the null hypothesis (often written $H_o$) is the default assumption that nothing interesting is going on.\n",
        "\n",
        "For a regression model, the null hypothesis for each predictor says: \"This predictor has no effect on the response. Or mathematically\n",
        "$$H_0 : \\beta = 0 $$\n",
        "\n",
        "The logic\n",
        "\n",
        "* Null hypothesis: coefficient = 0 (predictor has no effect).\n",
        "\n",
        "* Alternative hypothesis: coefficient â‰  0 (predictor has an effect).\n",
        "\n",
        "**When you run the regression:**\n",
        "\n",
        "The coefficient estimate is whatever your data suggests (maybe â€“0.04, maybe +0.06, etc.).\n",
        "\n",
        "The p-value tells you the probability of seeing a coefficient this far from 0 if the true coefficient were really 0.\n",
        "\n",
        "\n",
        "**The rule of thumb**\n",
        "\n",
        "If p < 0.05 â†’ strong enough evidence against the null â†’ predictor is statistically significant.\n",
        "\n",
        "If p â‰¥ 0.05 â†’ not enough evidence â†’ predictor is not significant.\n",
        "\n",
        "âš ï¸ Careful: The coefficient itself doesnâ€™t have to equal zero â€” itâ€™s the test against zero that matters.\n",
        "For example:\n",
        "\n",
        "Lag2 coefficient = 0.058, p = 0.03 â†’ âœ… significant.\n",
        "\n",
        "Lag1 coefficient = â€“0.041, p = 0.14 â†’ âŒ not significant, even though the estimate isnâ€™t exactly 0."
      ],
      "metadata": {
        "id": "TBjNf4MndTu8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#"
      ],
      "metadata": {
        "id": "rdE3vZc3sv5A"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lyStLVcNvqOH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}