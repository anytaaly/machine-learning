{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGYqoKcgSp/5oCRFEczIdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anytaaly/machine-learning/blob/main/Machine_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyPVM_Y33Jou"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction and Probability Review\n",
        "This module will introduce the main idea behind statistical learning for data science. You will learn to differentiate between model-driven and data-driven approaches to address complex problems, as well as supervised and unsupervised learning techniques. We will start by providing a crash course on probability tools that will be needed throughout the course, such as probability spaces, random variables, probability distributions, expectations, Bayes’ rule, and multivariate probability. Finally, we will introduce the programming language Python, which will be used to illustrate our theoretical advances throughout the course.\n",
        "\n",
        "# Learning Objectives\n",
        "\n",
        "Understand the difference between model-driven and data-driven approaches to\n",
        "\n",
        "*   Understand the difference between model-driven and data-driven approaches to address complex problems.\n",
        "*   Understand the difference between supervised and unsupervised learning problems.\n",
        "Remember the probability tools to study statistical learning problems in future modules.\n",
        "*  Install and explore the Python programming language using notebooks."
      ],
      "metadata": {
        "id": "xr4w4LmL-EF4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stochastic modeling** is a mathematical approach to represent and predict systems or phenomena that involve an element of randomness or chance"
      ],
      "metadata": {
        "id": "6fIznBsrBsHR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "This course will not focus on programming or implementation, for which you can find excellent courses online, but on developing an understanding about why, how and when some of these learning techniques work from a statistical perspective.\n",
        "\n",
        "Choosing the right learning technique to implement in your code must be based on an understanding of the tool you're implementing, instead of blindly trying different alternatives.\n",
        "\n",
        "On the other hand, those making decisions from data must understand the scope and limitations of the techniques used to digest this data in order to make informed and valuable decisions.\n"
      ],
      "metadata": {
        "id": "-pXQhY95BwQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data science is about drawing useful conclusions from large and diverse data sets through three phases, which I described here as exploration, prediction and inference."
      ],
      "metadata": {
        "id": "V1qGIDopCwsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- Exploration Phase:\n",
        "In the exploration phase, we try to identify patterns that are useful in our analysis using visualization and descriptive statistics.\n",
        "Also known as Exploratory Data Analysis (EDA), it helps uncover insights, identify anomalies and outliers, formulate hypotheses, and select appropriate analytical methods before building models or making conclusions."
      ],
      "metadata": {
        "id": "vdZuuU-LKg97"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 2- Planning Phase:\n",
        "In a second phase, which I call prediction, we use information to make informed guesses about values we wish we knew using machine learning and other tools."
      ],
      "metadata": {
        "id": "uv0zPM4nK1gb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Inference\n",
        "In our third phase called inference. We quantify the degree of certainty or uncertainty in our models. In other words, we try to answer the question, how accurate are our predictions? The main tools in this third phase are statistical tests and models.\n",
        "\n",
        "In data science, inference refers to the process of using data analysis and statistical methods to draw conclusions about a larger population or system based on a sample of data."
      ],
      "metadata": {
        "id": "sMYBAipqLGyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Statistics is a central component in data science because it studies how to make robust conclusions based on incomplete information, and this will be the focus of this course."
      ],
      "metadata": {
        "id": "fK-mR3VxLjUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "8p_GsCYWCZI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data-Driven Learning\n",
        "**1. Model-Based Approach (First-Principles Driven)**\n",
        "\n",
        "Definition: This approach relies on theoretical models derived from established scientific principles (e.g., Newton’s laws of motion, Maxwell’s equations, Einstein’s relativity).\n",
        "\n",
        "**Strengths:**\n",
        "\n",
        "Provides explainability (why something happens).\n",
        "\n",
        "Models often have predictive power in well-defined domains (physics, chemistry, engineering).\n",
        "\n",
        "Require relatively less data since the governing equations are already known.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "Breaks down in complex, chaotic, or poorly understood systems where exact governing laws are unknown or too difficult to model (e.g., climate change in detail, human behavior, stock markets).\n",
        "\n",
        "**2. Data-Driven Approach (Empirical / Machine Learning)**\n",
        "\n",
        "Definition: Instead of relying on known physical laws, this approach uses data itself to infer patterns, correlations, and predictive models.\n",
        "\n",
        "Examples:\n",
        "\n",
        "Stock market prediction (too many hidden variables).\n",
        "\n",
        "Human psychology and behavior modeling.\n",
        "\n",
        "Brain activity and neural processing.\n",
        "\n",
        "Strengths:\n",
        "\n",
        "Can uncover patterns in systems that are too complex or nonlinear for first-principle models.\n",
        "\n",
        "Improves as more data becomes available (big data + machine learning).\n",
        "\n",
        "Limitations:\n",
        "\n",
        "Often lacks interpretability—models may be “black boxes.”\n",
        "\n",
        "Requires large amounts of high-quality data.\n",
        "\n",
        "May capture correlation, not causation.\n",
        "These systems are so complex that we don't have the Newton slow equivalent.\n"
      ],
      "metadata": {
        "id": "eqxwbhZyBIdX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1WXt-RLUhV3kjLWHZTtSgYC2u3qTyS1vX)\n"
      ],
      "metadata": {
        "id": "mZfmM2oDSsgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1gYRBIsnNuwrnGQ3khC2Ptkd39v4Df0PZ)\n"
      ],
      "metadata": {
        "id": "JDge16MgF7i7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "X4huo1_HGj_y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Consider a collection of 1000 gray-scale images of dogs and cats. Each image has a resolution of 32 x 32 pixels, and each pixel has 256 possible gray levels. Your learning problem is to say what is the animal in an image. In this scenario, what is the number of features p and the number of data points N ?*\n",
        "\n",
        "Looking at this machine learning scenario, let me break down the components:\n",
        "Number of data points (N):\n",
        "N = 1000\n",
        "This is simply the total number of images in your collection.\n",
        "Number of features (p):\n",
        "Each image has 32 × 32 = 1,024 pixels, and if we treat each pixel as a feature, then:\n",
        "p = 1,024\n",
        "Each pixel can take one of 256 possible gray levels (typically 0-255), but the number of features is determined by the dimensionality of the input space, not the number of possible values each feature can take.\n",
        "So in summary:\n",
        "\n",
        "N = 1,000 (data points)\n",
        "p = 1,024 (features)\n",
        "\n",
        "This gives you a scenario where p ≈ N, which is interesting from a machine learning perspective since you're in a regime where the number of features is comparable to the number of training examples. This can present challenges like overfitting and may require techniques like regularization, dimensionality reduction, or data augmentation to achieve good generalization performance."
      ],
      "metadata": {
        "id": "LeXYU7DPVkKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zFoQg-VBVqe6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RnWYZ7DHVj6W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}